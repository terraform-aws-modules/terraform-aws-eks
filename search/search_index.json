{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Terraform AWS EKS module","text":"<p>Moar content coming soon!</p>"},{"location":"CHANGELOG.pre-v11.0.0/","title":"Change Log","text":"<p>All notable changes to this project will be documented in this file.</p> <p>The format is based on Keep a Changelog and this project adheres to Semantic Versioning.</p>"},{"location":"CHANGELOG.pre-v11.0.0/#v1000-2020-03-12","title":"v10.0.0 - 2020-03-12","text":"<p>BREAKING CHANGES:</p> <ul> <li>Added support for EKS 1.15 (by @sc250024)</li> </ul> <p>ENHANCEMENTS:</p> <ul> <li>Ensuring that ami lookup hierarchy is worker_group_launch_templates and worker_groups -&gt; worker_group_defaults -&gt; and finally aws ami lookup (by @ck3mp3r)</li> <li>Adding <code>encrypted</code> option to worker's root_block_device as read from the worker configurations (by @craig-rueda)</li> <li>Add support for ASG max instance lifetime (by @sidprak)</li> <li>Add <code>default_cooldown</code> and <code>health_check_grace_period</code> options to workers ASG (by @ArieLevs)</li> <li>Add support for envelope encryption of Secrets (by @babilen5)</li> </ul> <p>BUG FIXES:</p> <ul> <li>Fix issue with terraform plan phase when IRSA was enabled and create_eks switches to false (by @daroga0002)</li> <li>Remove obsolete assumption from README (kubectl &amp; aws-iam-authenticator) (by @pierresteiner)</li> <li>Fix doc about spot instances, cluster-autoscaler should be scheduled on normal instances instead of spot (by @simowaer)</li> <li>Use correct policy arns for CN regions (cn-north-1, cn-northwest-1) (by @cofyc)</li> <li>Fix support for ASG max instance lifetime for workers (by @barryib)</li> </ul> <p>NOTES:</p> <p>From EKS 1.15, the VPC tag <code>kubernetes.io/cluster/&lt;cluster-name&gt;: shared</code> is no longer required. So we droped those tags from exemples.</p>"},{"location":"CHANGELOG.pre-v11.0.0/#v900-2020-02-27","title":"v9.0.0 - 2020-02-27","text":"<ul> <li>Breaking: Removal of autoscaling IAM policy and tags (by @max-rocket-internet)</li> <li>Revert #631. Add back manage security group flags. (by @ryanooi)</li> <li>Changed timeout for creating EKS (by @confiq)</li> <li>Added instructions for how to add Windows nodes (by @ivanguravel)</li> <li>[CI] Switch <code>Validate</code> github action to use env vars (by @max-rocket-internet)</li> <li>[CI] Bump pre-commit-terraform version (by @barryib)</li> <li>Added example <code>examples/irsa</code> for IAM Roles for Service Accounts (by @max-rocket-internet)</li> <li>Add <code>iam:{Create,Delete,Get}OpenIDConnectProvider</code> grants to the list of required IAM permissions in <code>docs/iam-permissions.md</code> (by @danielelisi)</li> <li>Add a <code>name</code> parameter to be able to manually name EKS Managed Node Groups (by @splieth)</li> <li>Pinned kubernetes provider version to exactly 1.10.0 across all examples and README.md's (by @andres-de-castro)</li> <li>Change variable default <code>wait_for_cluster_cmd</code> from curl to wget (by @daroga0002)</li> </ul>"},{"location":"CHANGELOG.pre-v11.0.0/#important-notes","title":"Important notes","text":"<p>Autoscaling policy and tags have been removed from this module. This reduces complexity and increases security as the policy was attached to the node group IAM role. To manage it outside of this module either follow the example in <code>examples/irsa</code> to attach an IAM role to the cluster-autoscaler <code>serviceAccount</code> or create the policy outside this module and pass it in using the <code>workers_additional_policies</code> variable.</p>"},{"location":"CHANGELOG.pre-v11.0.0/#v820-2020-01-29","title":"v8.2.0 - 2020-01-29","text":"<ul> <li>Include ability to configure custom os-specific command for waiting until kube cluster is healthy (@sanjeevgiri)</li> <li>Disable creation of ingress rules if worker nodes security groups are exists (@andjelx)</li> <li>[CI] Update pre-commit and re-generate docs to work with terraform-docs &gt;= 0.8.1 (@barryib)</li> </ul>"},{"location":"CHANGELOG.pre-v11.0.0/#v810-2020-01-17","title":"v8.1.0 - 2020-01-17","text":"<ul> <li>Fix index reference on destroy for output <code>oidc_provider_arn</code> (@stevie-)</li> <li>Add support for restricting access to the public API endpoint (@sidprak)</li> <li>Add an <code>ignore_lifecycle</code> rule to prevent Terraform from scaling down ASG behind AWS EKS Managed Node Group (by @davidalger)</li> </ul>"},{"location":"CHANGELOG.pre-v11.0.0/#v800-2020-01-09","title":"v8.0.0 - 2020-01-09","text":"<ul> <li>Breaking: Change logic of security group whitelisting. Will always whitelist worker security group on control plane security group either provide one or create new one. See Important notes below for upgrade notes (by @ryanooi)</li> <li>Breaking: Configure the aws-auth configmap using the terraform kubernetes providers. See Important notes below for upgrade notes (by @sdehaes)</li> <li>Wait for cluster to respond to kubectl before applying auth map_config (@shaunc)</li> <li>Added flag <code>create_eks</code> to conditionally create resources (by @syst0m / @tbeijen)</li> <li>Support for AWS EKS Managed Node Groups. (by @wmorgan6796)</li> <li>Added a if check on <code>aws-auth</code> configmap when <code>map_roles</code> is empty (by @shanmugakarna)</li> <li>Removed no longer used variable <code>write_aws_auth_config</code> (by @tbeijen)</li> <li>Exit with error code when <code>aws-auth</code> configmap is unable to be updated (by @knittingdev)</li> <li>Fix deprecated interpolation-only expression (by @angelabad)</li> <li>Updated required version of AWS Provider to &gt;= v2.38.0 for Managed Node Groups (by @wmorgan6796)</li> <li>Updated minimum version of Terraform to avoid a bug (by @dpiddockcmp)</li> <li>Fix cluster_oidc_issuer_url output from list to string (by @chewvader)</li> <li>Fix idempotency issues for node groups with no remote_access configuration (by @jeffmhastings)</li> <li>Fix aws-auth config map for managed node groups (by @wbertelsen)</li> <li>Added support to create IAM OpenID Connect Identity Provider to enable EKS Identity Roles for Service Accounts (IRSA). (by @alaa)</li> <li>Adding node group iam role arns to outputs. (by @mukgupta)</li> <li>Added the OIDC Provider ARN to outputs. (by @eytanhanig)</li> <li>Move <code>eks_node_group</code> resources to a submodule (by @dpiddockcmp)</li> <li>Add complex output <code>node_groups</code> (by @TBeijen)</li> </ul>"},{"location":"CHANGELOG.pre-v11.0.0/#important-notes_1","title":"Important notes","text":"<p>The way the <code>aws-auth</code> configmap in the <code>kube-system</code> namespaces is managed has been changed. Before this was managed via kubectl using a null resources. This was changed to be managed by the terraform Kubernetes provider.</p> <p>To upgrade you have to add the kubernetes provider to the place you are calling the module. You can see examples in the examples folder. Then you should import the configmap into Terraform:</p> <pre><code>terraform import module.cluster1.kubernetes_config_map.aws_auth[0] kube-system/aws-auth\n</code></pre> <p>You could also delete the aws-auth config map before doing an apply but this means you need to the apply with the same user/role that created the cluster.</p> <p>For security group whitelisting change. After upgrade, have to remove <code>cluster_create_security_group</code> and <code>worker_create_security_group</code> variable. If you have whitelist worker security group before, you will have to delete it(and apply again) or import it.</p> <pre><code>terraform import module.eks.aws_security_group_rule.cluster_https_worker_ingress &lt;CONTROL_PLANE_SECURITY_GROUP_ID&gt;_ingress_tcp_443_443_&lt;WORKER_SECURITY_GROUP_ID&gt;\n</code></pre>"},{"location":"CHANGELOG.pre-v11.0.0/#v701-2019-12-11","title":"v7.0.1 - 2019-12-11","text":"<ul> <li>Test against minimum versions specified in <code>versions.tf</code> (by @dpiddockcmp)</li> <li>Updated <code>instance_profile_names</code> and <code>instance_profile_arns</code> outputs to also consider launch template as well as asg (by @ankitwal)</li> <li>Fix broken terraform plan/apply on a cluster &lt; 1.14 (by @hodduc)</li> <li>Updated application of <code>aws-auth</code> configmap to create <code>kube_config.yaml</code> and <code>aws_auth_configmap.yaml</code> in sequence (and not parallel) to <code>kubectl apply</code> (by @knittingdev)</li> </ul>"},{"location":"CHANGELOG.pre-v11.0.0/#v700-2019-10-30","title":"v7.0.0 - 2019-10-30","text":"<ul> <li>Breaking: Allow for specifying a custom AMI for the worker nodes. (by @bmcstdio)</li> <li>Added support for Windows workers AMIs (by @hodduc)</li> <li>Allow for replacing the full userdata text with a <code>userdata_template_file</code> template and <code>userdata_template_extra_args</code> in <code>worker_groups</code> (by @snstanton)</li> <li>Breaking: The <code>kubectl</code> configuration file can now be fully-specified using <code>config_output_path</code>. Previously it was assumed that <code>config_output_path</code> referred to a directory and always ended with a forward slash. This is a breaking change if <code>config_output_path</code> does not end with a forward slash (which was advised against by the documentation). (by @joshuaspence)</li> <li>Changed logic for setting default <code>ebs_optimized</code> to only require maintaining a list of instance types that don't support it (by @jeffmhastings)</li> <li>Bumped minimum terraform version to 0.12.2 to prevent an error on yamlencode function (by @toadjaune)</li> <li>Access conditional resource using join function in combination with splat syntax (by @miguelaferreira)</li> </ul>"},{"location":"CHANGELOG.pre-v11.0.0/#important-notes_2","title":"Important notes","text":"<p>An AMI is now specified using the whole name, for example <code>amazon-eks-node-1.14-v20190927</code>.</p>"},{"location":"CHANGELOG.pre-v11.0.0/#v602-2019-10-07","title":"v6.0.2 - 2019-10-07","text":"<ul> <li>Added <code>tags</code> to <code>aws_eks_cluster</code> introduced by terraform-provider-aws 2.31.0 (by @morganchristiansson)</li> <li>Add option to enable lifecycle hooks creation (by @barryib)</li> <li>Remove helm chart value <code>sslCertPath</code> described in <code>docs/autoscaling.md</code> (by @wi1dcard)</li> <li>Attaching of IAM policies for autoscaler and CNI to the worker nodes now optional (by @dpiddockcmp)</li> </ul>"},{"location":"CHANGELOG.pre-v11.0.0/#v601-2019-09-25","title":"v6.0.1 - 2019-09-25","text":"<ul> <li>Added support for different workers AMI's, i.e. with GPU support (by @rvoitenko)</li> <li>Use null as default value for <code>target_group_arns</code> attribute of worker autoscaling group (by @tatusl)</li> <li>Output empty string when cluster identity is empty (by @tbarry)</li> </ul>"},{"location":"CHANGELOG.pre-v11.0.0/#v600-2019-09-17","title":"v6.0.0 - 2019-09-17","text":"<ul> <li>Added <code>market_type</code> to <code>workers_launch_template.tf</code> allow the usage of spot nodegroups without mixed instances policy.</li> <li>Added support for log group tag in <code>./cluster.tf</code> (@lucas-giaco)</li> <li>Added support for workers iam role tag in <code>./workers.tf</code> (@lucas-giaco)</li> <li>Added <code>required_providers</code> to enforce provider minimum versions (by @dpiddockcmp)</li> <li>Updated <code>local.spot_allocation_strategy</code> docstring to indicate availability of new <code>capacity-optimized</code> option. (by @sc250024)</li> <li>Added support for initial lifecycle hooks for autosacling groups (@barryib)</li> <li>Added option to recreate ASG when LT or LC changes (by @barryib)</li> <li>Ability to specify workers role name (by @ivanich)</li> <li>Added output for OIDC Issuer URL (by @russwhelan)</li> <li>Added support for Mixed Instance ASG using <code>worker_groups_launch_template</code> variable  (by @sppwf)</li> <li>Changed ASG Tags generation using terraform 12 <code>for</code> utility  (by @sppwf)</li> <li>Breaking: Removed <code>worker_groups_launch_template_mixed</code> variable (by @sppwf)</li> <li>Update to EKS 1.14 (by @nauxliu)</li> <li>Breaking: Support map users and roles to multiple groups (by @nauxliu)</li> <li>Fixed errors sometimes happening during destroy due to usage of coalesce() in local.tf (by @petrikero)</li> <li>Removed historical mention of adding caller's IPv4 to cluster security group (by @dpiddockcmp)</li> <li>Wrapped <code>kubelet_extra_args</code> in double quotes instead of singe quotes (by @nxf5025)</li> <li>Make terraform plan more consistent and avoid unnecessary \"(known after apply)\" (by @barryib)</li> <li>Made sure that <code>market_type</code> was correctly passed to <code>workers_launch_template</code> (by @to266)</li> </ul>"},{"location":"CHANGELOG.pre-v11.0.0/#important-notes_3","title":"Important notes","text":"<p>You will need to move worker groups from <code>worker_groups_launch_template_mixed</code> to <code>worker_groups_launch_template</code>. You can rename terraform resources in the state to avoid an destructive changes.</p> <p>Map roles need to rename <code>role_arn</code> to <code>rolearn</code> and <code>group = \"\"</code> to <code>groups = [\"\"]</code>.</p>"},{"location":"CHANGELOG.pre-v11.0.0/#v511-2019-07-30","title":"v5.1.1 - 2019-07-30","text":"<ul> <li>Added new tag in <code>worker.tf</code> with autoscaling_enabled = true flag (by @insider89)</li> </ul>"},{"location":"CHANGELOG.pre-v11.0.0/#v510-2019-07-30","title":"v5.1.0 - 2019-07-30","text":"<ul> <li>Option to set a KMS key for the log group and encrypt it (by @till-krauss)</li> <li>Output the name of the cloudwatch log group (by @gbooth27)</li> <li>Added <code>cpu_credits</code> param for the workers defined in <code>worker_groups_launch_template</code> (by @a-shink)</li> <li>Added support for EBS Volumes tag in <code>worker_groups_launch_template</code> and <code>workers_launch_template_mixed.tf</code> (by @sppwf)</li> <li>Basic example now tags networks correctly, as per ELB documentation and ALB documentation (by @karolinepauls)</li> <li>Update default override instance types to work with Cluster Autoscaler (by @nauxliu on behalf of RightCapital)</li> <li>Examples now specify <code>enable_dns_hostnames = true</code>, as per EKS documentation (by @karolinepauls)</li> </ul>"},{"location":"CHANGELOG.pre-v11.0.0/#v500-2019-06-19","title":"v5.0.0 - 2019-06-19","text":"<ul> <li>Added Termination Policy Option to worker ASGs (by @undeadops)</li> <li>Update EBS optimized instances type (by @gloutsch)</li> <li>Added tagging for iam role created in <code>./cluster.tf</code> (@camilosantana)</li> <li>Enable log retention for cloudwatch log groups (by @yuriipolishchuk)</li> <li>Update to EKS 1.13 (by @gloutsch)</li> <li>Finally, Terraform 0.12 support, Upgrade Guide (by @alex-goncharov @nauxliu @timboven)</li> <li>All the xx_count variables have been removed (by @nauxliu on behalf of RightCapital)</li> <li>Use actual lists in the workers group maps instead of strings with commas (by @nauxliu on behalf of RightCapital)</li> <li>Move variable <code>worker_group_tags</code> to workers group's attribute <code>tags</code> (by @nauxliu on behalf of RightCapital)</li> <li>Change override instance_types to list (by @nauxliu on behalf of RightCapital)</li> <li>Fix toggle for IAM instance profile creation for mixed launch templates (by @jnozo)</li> </ul>"},{"location":"CHANGELOG.pre-v11.0.0/#v402-2019-05-07","title":"v4.0.2 - 2019-05-07","text":"<ul> <li>Added 2 new examples, also tidy up basic example (by @max-rocket-internet)</li> <li>Updates to travis, PR template (by @max-rocket-internet)</li> <li>Fix typo in data.tf (by @max-rocket-internet)</li> <li>Add missing launch template items in <code>aws_auth.tf</code> (by @max-rocket-internet)</li> </ul>"},{"location":"CHANGELOG.pre-v11.0.0/#v401-2019-05-07","title":"v4.0.1 - 2019-05-07","text":"<ul> <li>Fix annoying typo: worker_group_xx vs worker_groups_xx (by @max-rocket-internet)</li> </ul>"},{"location":"CHANGELOG.pre-v11.0.0/#v400-2019-05-07","title":"v4.0.0 - 2019-05-07","text":"<ul> <li>Added support for custom service linked role for Auto Scaling group (by @voanhduy1512)</li> <li>Added support for custom IAM roles for cluster and workers (by @erks)</li> <li>Added cluster ARN to outputs (by @alexsn)</li> <li>Added outputs for <code>workers_user_data</code> and <code>workers_default_ami_id</code> (by @max-rocket-internet)</li> <li>Added doc about spot instances (by @max-rocket-internet)</li> <li>Added new worker group option with a mixed instances policy (by @max-rocket-internet)</li> <li>Set default suspended processes for ASG to <code>AZRebalance</code> (by @max-rocket-internet)</li> <li>4 small changes to <code>aws_launch_template</code> resource (by @max-rocket-internet)</li> <li>(Breaking Change) Rewritten and de-duplicated code related to Launch Templates (by @max-rocket-internet)</li> <li>Add .prettierignore file (by @rothandrew)</li> <li>Switch to https for the pre-commit repos (by @rothandrew)</li> <li>Add instructions on how to enable the docker bridge network (by @rothandrew)</li> </ul>"},{"location":"CHANGELOG.pre-v11.0.0/#v300-2019-04-15","title":"v3.0.0 - 2019-04-15","text":"<ul> <li>Fixed: Ability to destroy clusters due to security groups being attached to ENI's (by @whiskeyjimbo)</li> <li>Added outputs for worker IAM instance profile(s) (by @soapergem)</li> <li>Added support for cluster logging via the <code>cluster_enabled_log_types</code> variable (by @sc250024)</li> <li>Updated vpc module version and aws provider version. (by @chenrui333)</li> <li>Upgraded default kubernetes version from 1.11 to 1.12 (by @stijndehaes)</li> </ul>"},{"location":"CHANGELOG.pre-v11.0.0/#v231-2019-03-26","title":"v2.3.1 - 2019-03-26","text":"<ul> <li>Added support for eks public and private endpoints (by @stijndehaes)</li> <li>Added minimum inbound traffic rule to the cluster worker security group as per the EKS security group requirements (by @sc250024)</li> <li>(Breaking Change) Replaced <code>enable_docker_bridge</code> with a generic option called <code>bootstrap_extra_args</code> to resolve 310 (by @max-rocket-internet)</li> </ul>"},{"location":"CHANGELOG.pre-v11.0.0/#v230-2019-03-20","title":"v2.3.0 - 2019-03-20","text":"<ul> <li>Allow additional policies to be attached to worker nodes (by @rottenbytes)</li> <li>Ability to specify a placement group for each worker group (by @matheuss)</li> <li>\"k8s.io/cluster-autoscaler/{cluster-name}\" and \"k8s.io/cluster-autoscaler/node-template/resources/ephemeral-storage\" tags for autoscaling groups (by @tbarrella)</li> <li>Added \"ec2:DescribeLaunchTemplateVersions\" action to worker instance role (by @skang0601)</li> <li>Adding ebs encryption for workers launched using workers_launch_template (by @russki)</li> <li>Added output for generated kubeconfig filename (by @syst0m)</li> <li>Added outputs for cluster role ARN and name (by @spingel)</li> <li>Added optional name filter variable to be able to pin worker AMI to a release (by @max-rocket-internet)</li> <li>Added <code>--enable-docker-bridge</code> option for bootstrap.sh in AMI (by @michaelmccord)</li> </ul>"},{"location":"CHANGELOG.pre-v11.0.0/#v222-2019-02-25","title":"v2.2.2 - 2019-02-25","text":"<ul> <li>Ability to specify a path for IAM roles (by @tekn0ir)</li> </ul>"},{"location":"CHANGELOG.pre-v11.0.0/#v221-2019-02-18","title":"v2.2.1 - 2019-02-18","text":""},{"location":"CHANGELOG.pre-v11.0.0/#v220-2019-02-07","title":"v2.2.0 - 2019-02-07","text":"<ul> <li>Ability to specify a permissions_boundary for IAM roles (by @dylanhellems)</li> <li>Ability to configure force_delete for the worker group ASG (by @stefansedich)</li> <li>Ability to configure worker group ASG tags (by @stefansedich)</li> <li>Added EBS optimized mapping for the g3s.xlarge instance type (by @stefansedich)</li> <li><code>enabled_metrics</code> input (by @zanitete)</li> <li>write_aws_auth_config to input (by @yutachaos)</li> <li>Change worker group ASG to use create_before_destroy (by @stefansedich)</li> <li>Fixed a bug where worker group defaults were being used for launch template user data (by @leonsodhi-lf)</li> <li>Managed_aws_auth option is true, the aws-auth configmap file is no longer created, and write_aws_auth_config must be set to true to generate config_map. (by @yutachaos)</li> </ul>"},{"location":"CHANGELOG.pre-v11.0.0/#v210-2019-01-15","title":"v2.1.0 - 2019-01-15","text":"<ul> <li>Initial support for worker groups based on Launch Templates (by @skang0601)</li> <li>Updated the <code>update_config_map_aws_auth</code> resource to trigger when the EKS cluster endpoint changes. This likely means that a new cluster was spun up so our ConfigMap won't exist (fixes #234) (by @elatt)</li> <li>Removed invalid action from worker_autoscaling iam policy (by @marcelloromani)</li> <li>Fixed zsh-specific syntax in retry loop for aws auth config map (by @marcelloromani)</li> <li>Fix: fail deployment if applying the aws auth config map still fails after 10 attempts (by @marcelloromani)</li> </ul>"},{"location":"CHANGELOG.pre-v11.0.0/#v200-2018-12-14","title":"v2.0.0 - 2018-12-14","text":"<ul> <li>(Breaking Change) New input variables <code>map_accounts_count</code>, <code>map_roles_count</code> and <code>map_users_count</code> to allow using computed values as part of <code>map_accounts</code>, <code>map_roles</code> and <code>map_users</code> configs (by @chili-man on behalf of OpenGov).</li> <li>(Breaking Change) New variables <code>cluster_create_security_group</code> and <code>worker_create_security_group</code> to stop <code>value of 'count' cannot be computed</code> error.</li> <li>Added ability to choose local-exec interpreter (by @rothandrew)</li> <li>Added <code>--with-aggregate-type-defaults</code> option to terraform-docs (by @max-rocket-internet)</li> <li>Updated AMI ID filtering to only filter AMIs from current cluster k8s version (by @max-rocket-internet)</li> <li>Added <code>pre-commit-terraform</code> git hook to automatically create documentation of inputs/outputs (by @antonbabenko)</li> <li>Travis fixes (by @RothAndrew)</li> <li>Fixed some Windows compatibility issues (by @RothAndrew)</li> </ul>"},{"location":"CHANGELOG.pre-v11.0.0/#v180-2018-12-04","title":"v1.8.0 - 2018-12-04","text":"<ul> <li>Support for using AWS Launch Templates to define autoscaling groups (by @skang0601)</li> <li><code>suspended_processes</code> to <code>worker_groups</code> input (by @bkmeneguello)</li> <li><code>target_group_arns</code> to <code>worker_groups</code> input (by @zihaoyu)</li> <li><code>force_detach_policies</code> to <code>aws_iam_role</code> <code>cluster</code> and <code>workers</code> (by @marky-mark)</li> <li>Added sleep while trying to apply the kubernetes configurations if failed, up to 50 seconds (by @rmakram-ims)</li> <li><code>cluster_create_security_group</code> and <code>worker_create_security_group</code>. This allows using computed cluster and worker security groups. (by @rmakram-ims)</li> <li>new variables worker_groups_launch_template and worker_group_count_launch_template (by @skang0601)</li> <li>Remove aws_iam_service_linked_role (by @max-rocket-internet)</li> <li>Adjust the order and correct/update the ec2 instance type info. (@chenrui333)</li> <li>Removed providers from <code>main.tf</code>. (by @max-rocket-internet)</li> <li>Removed <code>configure_kubectl_session</code> references in documentation #171 (by @dominik-k)</li> </ul>"},{"location":"CHANGELOG.pre-v11.0.0/#v170-2018-10-09","title":"v1.7.0 - 2018-10-09","text":"<ul> <li>Worker groups can be created with a specified IAM profile. (from @laverya)</li> <li>exposed <code>aws_eks_cluster</code> create and destroy timeouts (by @RGPosadas)</li> <li>exposed <code>placement_tenancy</code> for autoscaling group (by @monsterxx03)</li> <li>Allow port 443 from EKS service to nodes to run <code>metrics-server</code>. (by @max-rocket-internet)</li> <li>fix default worker subnets not working (by @erks)</li> <li>fix default worker autoscaling_enabled not working (by @erks)</li> <li>Cosmetic syntax changes to improve readability. (by @max-rocket-internet)</li> <li>add <code>protect_from_scale_in</code> to solve issue #134 (by @kinghajj)</li> </ul>"},{"location":"CHANGELOG.pre-v11.0.0/#v160-2018-09-04","title":"v1.6.0 - 2018-09-04","text":"<ul> <li>add support for <code>amazon-eks-node-*</code> AMI with bootstrap script (by @erks)</li> <li>expose <code>kubelet_extra_args</code> worker group option (replacing <code>kubelet_node_labels</code>) to allow specifying arbitrary kubelet options (e.g. taints and labels) (by @erks)</li> <li>add optional input <code>worker_additional_security_group_ids</code> to allow one or more additional security groups to be added to all worker launch configurations - #47 (by @hhobbsh @mr-joshua)</li> <li>add optional input <code>additional_security_group_ids</code> to allow one or more additional security groups to be added to a specific worker launch configuration - #47 (by @mr-joshua)</li> <li>allow a custom AMI to be specified as a default (by @erks)</li> <li>bugfix for above change (by @max-rocket-internet)</li> <li>Breaking change Removed support for <code>eks-worker-*</code> AMI. The cluster specifying a custom AMI based off of <code>eks-worker-*</code> AMI will have to rebuild the AMI from <code>amazon-eks-node-*</code>.  (by @erks)</li> <li>Breaking change Removed <code>kubelet_node_labels</code> worker group option in favor of <code>kubelet_extra_args</code>. (by @erks)</li> </ul>"},{"location":"CHANGELOG.pre-v11.0.0/#v150-2018-08-30","title":"v1.5.0 - 2018-08-30","text":"<ul> <li>add spot_price option to aws_launch_configuration</li> <li>add enable_monitoring option to aws_launch_configuration</li> <li>add t3 instance class settings</li> <li>add aws_iam_service_linked_role for elasticloadbalancing. (by @max-rocket-internet)</li> <li>Added autoscaling policies into module that are optionally attached when enabled for a worker group. (by @max-rocket-internet)</li> <li>Breaking change Removed <code>workstation_cidr</code> variable, http callout and unnecessary security rule. (by @dpiddockcmp)   If you are upgrading from 1.4 you should fix state after upgrade: <code>terraform state rm module.eks.data.http.workstation_external_ip</code></li> <li>Can now selectively override keys in <code>workers_group_defaults</code> variable rather than callers maintaining a duplicate of the whole map. (by @dpiddockcmp)</li> </ul>"},{"location":"CHANGELOG.pre-v11.0.0/#v140-2018-08-02","title":"v1.4.0 - 2018-08-02","text":"<ul> <li>manage eks workers' root volume size and type.</li> <li><code>workers_asg_names</code> added to outputs. (kudos to @laverya)</li> <li>New top level variable <code>worker_group_count</code> added to replace the use of <code>length(var.worker_groups)</code>. This allows using computed values as part of worker group configs. (complaints to @laverya)</li> </ul>"},{"location":"CHANGELOG.pre-v11.0.0/#v130-2018-07-11","title":"v1.3.0 - 2018-07-11","text":"<ul> <li>New variables <code>map_accounts</code>, <code>map_roles</code> and <code>map_users</code> in order to manage additional entries in the <code>aws-auth</code> configmap. (by @max-rocket-internet)</li> <li>kubelet_node_labels worker group option allows setting --node-labels= in kubelet. (Hat-tip, @bshelton229 \ud83d\udc52)</li> <li><code>worker_iam_role_arn</code> added to outputs. Sweet, @hatemosphere \ud83d\udd25</li> <li>Worker subnets able to be specified as a dedicated list per autoscaling group. (up top, @bshelton229 \ud83d\ude4f)</li> </ul>"},{"location":"CHANGELOG.pre-v11.0.0/#v120-2018-07-01","title":"v1.2.0 - 2018-07-01","text":"<ul> <li>new variable <code>pre_userdata</code> added to worker launch configuration allows to run scripts before the plugin does anything. (W00t, @jimbeck \ud83e\udd89)</li> <li>kubeconfig made much more flexible. (Bang up job, @sdavids13 \ud83d\udca5)</li> <li>ASG desired capacity is now ignored as ASG size is more effectively handed by k8s. (Thanks, @ozbillwang \ud83d\udc87\u200d\u2642\ufe0f)</li> <li>Providing security groups didn't behave as expected. This has been fixed. (Good catch, @jimbeck \ud83d\udd27)</li> <li>workstation cidr to be allowed by created security group is now more flexible. (A welcome addition, @jimbeck \ud83d\udd10)</li> </ul>"},{"location":"CHANGELOG.pre-v11.0.0/#v110-2018-06-25","title":"v1.1.0 - 2018-06-25","text":"<ul> <li>new variable <code>worker_sg_ingress_from_port</code> allows to change the minimum port number from which pods will accept communication (Thanks, @ilyasotkov \ud83d\udc4f).</li> <li>expanded on worker example to show how multiple worker autoscaling groups can be created.</li> <li>IPv4 is used explicitly to resolve testing from IPv6 networks (thanks, @tsub \ud83d\ude4f).</li> <li>Configurable public IP attachment and ssh keys for worker groups. Defaults defined in <code>worker_group_defaults</code>. Nice, @hatemosphere \ud83c\udf02</li> <li><code>worker_iam_role_name</code> now an output. Sweet, @artursmet \ud83d\udd76\ufe0f</li> <li>IAM test role repaired by @lcharkiewicz \ud83d\udc85</li> <li><code>kube-proxy</code> restart no longer needed in userdata. Good catch, @hatemosphere \ud83d\udd25</li> <li>worker ASG reattachment wasn't possible when using <code>name</code>. Moved to <code>name_prefix</code> to allow recreation of resources. Kudos again, @hatemosphere \ud83d\udc27</li> </ul>"},{"location":"CHANGELOG.pre-v11.0.0/#v100-2018-06-11","title":"v1.0.0 - 2018-06-11","text":"<ul> <li>security group id can be provided for either/both of the cluster and the workers. If not provided, security groups will be created with sufficient rules to allow cluster-worker communication. - kudos to @tanmng on the idea \u2b50</li> <li>outputs of security group ids and worker ASG arns added for working with these resources outside the module.</li> <li>Worker build out refactored to allow multiple autoscaling groups each having differing specs. If none are given, a single ASG is created with a set of sane defaults - big thanks to @kppullin \ud83e\udd68</li> </ul>"},{"location":"CHANGELOG.pre-v11.0.0/#v020-2018-06-08","title":"v0.2.0 - 2018-06-08","text":"<ul> <li>ability to specify extra userdata code to execute following kubelet services start.</li> <li>EBS optimization used whenever possible for the given instance type.</li> <li>When <code>configure_kubectl_session</code> is set to true the current shell will be configured to talk to the kubernetes cluster using config files output from the module.</li> <li>files rendered from dedicated templates to separate out raw code and config from <code>hcl</code></li> <li><code>workers_ami_id</code> is now made optional. If not specified, the module will source the latest AWS supported EKS AMI instead.</li> </ul>"},{"location":"CHANGELOG.pre-v11.0.0/#v011-2018-06-07","title":"v0.1.1 - 2018-06-07","text":"<ul> <li>Pre-commit hooks fixed and working.</li> <li>Made progress on CI, advancing the build to the final <code>kitchen test</code> stage before failing.</li> </ul>"},{"location":"CHANGELOG.pre-v11.0.0/#v010-2018-06-07","title":"[v0.1.0] - 2018-06-07","text":"<ul> <li>Everything! Initial release of the module.</li> <li>added a local variable to do a lookup against for a dynamic value in userdata which was previously static. Kudos to @tanmng for finding and fixing bug #1!</li> </ul>"},{"location":"UPGRADE-17.0/","title":"How to handle the terraform-aws-eks module upgrade","text":""},{"location":"UPGRADE-17.0/#upgrade-module-to-v1700-for-managed-node-groups","title":"Upgrade module to v17.0.0 for Managed Node Groups","text":"<p>In this release, we now decided to remove random_pet resources in Managed Node Groups (MNG). Those were used to recreate MNG if something changed. But they were causing a lot of issues. To upgrade the module without recreating your MNG, you will need to explicitly reuse their previous name and set them in your MNG <code>name</code> argument.</p> <ol> <li>Run <code>terraform apply</code> with the module version v16.2.0</li> <li>Get your worker group names</li> </ol> <pre><code>~ terraform state show 'module.eks.module.node_groups.aws_eks_node_group.workers[\"example\"]' | grep node_group_name\nnode_group_name = \"test-eks-mwIwsvui-example-sincere-squid\"\n</code></pre> <ol> <li>Upgrade your module and configure your node groups to use existing names</li> </ol> <pre><code>module \"eks\" {\n  source  = \"terraform-aws-modules/eks/aws\"\n  version = \"17.0.0\"\n\n  cluster_name    = \"test-eks-mwIwsvui\"\n  cluster_version = \"1.20\"\n  # ...\n\n  node_groups = {\n    example = {\n      name = \"test-eks-mwIwsvui-example-sincere-squid\"\n\n      # ...\n    }\n  }\n  # ...\n}\n</code></pre> <ol> <li>Run <code>terraform plan</code>, you should see that only <code>random_pets</code> will be destroyed</li> </ol> <pre><code>Terraform will perform the following actions:\n\n  # module.eks.module.node_groups.random_pet.node_groups[\"example\"] will be destroyed\n  - resource \"random_pet\" \"node_groups\" {\n      - id        = \"sincere-squid\" -&gt; null\n      - keepers   = {\n          - \"ami_type\"                  = \"AL2_x86_64\"\n          - \"capacity_type\"             = \"SPOT\"\n          - \"disk_size\"                 = \"50\"\n          - \"iam_role_arn\"              = \"arn:aws:iam::123456789123:role/test-eks-mwIwsvui20210527220853611600000009\"\n          - \"instance_types\"            = \"t3.large\"\n          - \"key_name\"                  = \"\"\n          - \"node_group_name\"           = \"test-eks-mwIwsvui-example\"\n          - \"source_security_group_ids\" = \"\"\n          - \"subnet_ids\"                = \"subnet-xxxxxxxxxxxx|subnet-xxxxxxxxxxxx|subnet-xxxxxxxxxxxx\"\n        } -&gt; null\n      - length    = 2 -&gt; null\n      - separator = \"-\" -&gt; null\n    }\n\nPlan: 0 to add, 0 to change, 1 to destroy.\n</code></pre> <ol> <li>If everything sounds good to you, run <code>terraform apply</code></li> </ol> <p>After the first apply, we recommend you to create a new node group and let the module use the <code>node_group_name_prefix</code> (by removing the <code>name</code> argument) to generate names and avoid collision during node groups re-creation if needed, because the lifecycle is <code>create_before_destroy = true</code>.</p>"},{"location":"UPGRADE-18.0/","title":"Upgrade from v17.x to v18.x","text":"<p>Please consult the <code>examples</code> directory for reference example configurations. If you find a bug, please open an issue with supporting configuration to reproduce.</p> <p>Note: please see https://github.com/terraform-aws-modules/terraform-aws-eks/issues/1744 where users have shared the steps/changes that have worked for their configurations to upgrade. Due to the numerous configuration possibilities, it is difficult to capture specific steps that will work for all; this has proven to be a useful thread to share collective information from the broader community regarding v18.x upgrades.</p> <p>For most users, adding the following to your v17.x configuration will preserve the state of your cluster control plane when upgrading to v18.x:</p> <pre><code>prefix_separator                   = \"\"\niam_role_name                      = $CLUSTER_NAME\ncluster_security_group_name        = $CLUSTER_NAME\ncluster_security_group_description = \"EKS cluster security group.\"\n</code></pre> <p>This configuration assumes that <code>create_iam_role</code> is set to <code>true</code>, which is the default value.</p> <p>As the location of the Terraform state of the IAM role has been changed from 17.x to 18.x, you'll also have to move the state before running <code>terraform apply</code> by calling:</p> <pre><code>terraform state mv 'module.eks.aws_iam_role.cluster[0]' 'module.eks.aws_iam_role.this[0]'\n</code></pre> <p>See more information here</p>"},{"location":"UPGRADE-18.0/#list-of-backwards-incompatible-changes","title":"List of backwards incompatible changes","text":"<ul> <li>Launch configuration support has been removed and only launch template is supported going forward. AWS is no longer adding new features back into launch configuration and their docs state <code>We strongly recommend that you do not use launch configurations. They do not provide full functionality for Amazon EC2 Auto Scaling or Amazon EC2. We provide information about launch configurations for customers who have not yet migrated from launch configurations to launch templates.</code></li> <li>Support for managing aws-auth configmap has been removed. This change also removes the dependency on the Kubernetes Terraform provider, the local dependency on aws-iam-authenticator for users, as well as the reliance on the forked http provider to wait and poll on cluster creation. To aid users in this change, an output variable <code>aws_auth_configmap_yaml</code> has been provided which renders the aws-auth configmap necessary to support at least the IAM roles used by the module (additional mapRoles/mapUsers definitions to be provided by users)</li> <li>Support for managing kubeconfig and its associated <code>local_file</code> resources have been removed; users are able to use the awscli provided <code>aws eks update-kubeconfig --name &lt;cluster_name&gt;</code> to update their local kubeconfig as necessary</li> <li>The terminology used in the module has been modified to reflect that used by the AWS documentation.</li> <li>AWS EKS Managed Node Group, <code>eks_managed_node_groups</code>, was previously referred to as simply node group, <code>node_groups</code></li> <li>Self Managed Node Group Group, <code>self_managed_node_groups</code>, was previously referred to as worker group, <code>worker_groups</code></li> <li>AWS Fargate Profile, <code>fargate_profiles</code>, remains unchanged in terms of naming and terminology</li> <li>The three different node group types supported by AWS and the module have been refactored into standalone sub-modules that are both used by the root <code>eks</code> module as well as available for individual, standalone consumption if desired.</li> <li>The previous <code>node_groups</code> sub-module is now named <code>eks-managed-node-group</code> and provisions a single AWS EKS Managed Node Group per sub-module definition (previous version utilized <code>for_each</code> to create 0 or more node groups)<ul> <li>Additional changes for the <code>eks-managed-node-group</code> sub-module over the previous <code>node_groups</code> module include:</li> <li>Variable name changes defined in section <code>Variable and output changes</code> below</li> <li>Support for nearly full control of the IAM role created, or provide the ARN of an existing IAM role, has been added</li> <li>Support for nearly full control of the security group created, or provide the ID of an existing security group, has been added</li> <li>User data has been revamped and all user data logic moved to the <code>_user_data</code> internal sub-module; the local <code>userdata.sh.tpl</code> has been removed entirely</li> </ul> </li> <li>The previous <code>fargate</code> sub-module is now named <code>fargate-profile</code> and provisions a single AWS EKS Fargate Profile per sub-module definition (previous version utilized <code>for_each</code> to create 0 or more profiles)<ul> <li>Additional changes for the <code>fargate-profile</code> sub-module over the previous <code>fargate</code> module include:</li> <li>Variable name changes defined in section <code>Variable and output changes</code> below</li> <li>Support for nearly full control of the IAM role created, or provide the ARN of an existing IAM role, has been added</li> <li>Similar to the <code>eks_managed_node_group_defaults</code> and <code>self_managed_node_group_defaults</code>, a <code>fargate_profile_defaults</code> has been provided to allow users to control the default configurations for the Fargate profiles created</li> </ul> </li> <li>A sub-module for <code>self-managed-node-group</code> has been created and provisions a single self managed node group (autoscaling group) per sub-module definition<ul> <li>Additional changes for the <code>self-managed-node-group</code> sub-module over the previous <code>node_groups</code> variable include:</li> <li>The underlying autoscaling group and launch template have been updated to more closely match that of the <code>terraform-aws-autoscaling</code> module and the features it offers</li> <li>The previous iteration used a count over a list of node group definitions which was prone to disruptive updates; this is now replaced with a map/for_each to align with that of the EKS managed node group and Fargate profile behaviors/style</li> </ul> </li> <li>The user data configuration supported across the module has been completely revamped. A new <code>_user_data</code> internal sub-module has been created to consolidate all user data configuration in one location which provides better support for testability (via the <code>tests/user-data</code> example). The new sub-module supports nearly all possible combinations including the ability to allow users to provide their own user data template which will be rendered by the module. See the <code>tests/user-data</code> example project for the full plethora of example configuration possibilities and more details on the logic of the design can be found in the <code>modules/_user_data</code> directory.</li> <li>Resource name changes may cause issues with existing resources. For example, security groups and IAM roles cannot be renamed, they must be recreated. Recreation of these resources may also trigger a recreation of the cluster. To use the legacy (&lt; 18.x) resource naming convention, set <code>prefix_separator</code> to \"\".</li> <li>Security group usage has been overhauled to provide only the bare minimum network connectivity required to launch a bare bones cluster. See the security group documentation section for more details. Users upgrading to v18.x will want to review the rules they have in place today versus the rules provisioned by the v18.x module and ensure to make any necessary adjustments for their specific workload.</li> </ul>"},{"location":"UPGRADE-18.0/#additional-changes","title":"Additional changes","text":""},{"location":"UPGRADE-18.0/#added","title":"Added","text":"<ul> <li>Support for AWS EKS Addons has been added</li> <li>Support for AWS EKS Cluster Identity Provider Configuration has been added</li> <li>AWS Terraform provider minimum required version has been updated to 3.64 to support the changes made and additional resources supported</li> <li>An example <code>user_data</code> project has been added to aid in demonstrating, testing, and validating the various methods of configuring user data with the <code>_user_data</code> sub-module as well as the root <code>eks</code> module</li> <li>Template for rendering the aws-auth configmap output - <code>aws_auth_cm.tpl</code></li> <li>Template for Bottlerocket OS user data bootstrapping - <code>bottlerocket_user_data.tpl</code></li> </ul>"},{"location":"UPGRADE-18.0/#modified","title":"Modified","text":"<ul> <li>The previous <code>fargate</code> example has been renamed to <code>fargate_profile</code></li> <li>The previous <code>irsa</code> and <code>instance_refresh</code> examples have been merged into one example <code>irsa_autoscale_refresh</code></li> <li>The previous <code>managed_node_groups</code> example has been renamed to <code>self_managed_node_group</code></li> <li>The previously hardcoded EKS OIDC root CA thumbprint value and variable has been replaced with a <code>tls_certificate</code> data source that refers to the cluster OIDC issuer url. Thumbprint values should remain unchanged however</li> <li>Individual cluster security group resources have been replaced with a single security group resource that takes a map of rules as input. The default ingress/egress rules have had their scope reduced in order to provide the bare minimum of access to permit successful cluster creation and allow users to opt in to any additional network access as needed for a better security posture. This means the <code>0.0.0.0/0</code> egress rule has been removed, instead TCP/443 and TCP/10250 egress rules to the node group security group are used instead</li> <li>The Linux/bash user data template has been updated to include the bare minimum necessary for bootstrapping AWS EKS Optimized AMI derivative nodes with provisions for providing additional user data and configurations; was named <code>userdata.sh.tpl</code> and is now named <code>linux_user_data.tpl</code></li> <li>The Windows user data template has been renamed from <code>userdata_windows.tpl</code> to <code>windows_user_data.tpl</code></li> </ul>"},{"location":"UPGRADE-18.0/#removed","title":"Removed","text":"<ul> <li>Miscellaneous documents on how to configure Kubernetes cluster internals have been removed. Documentation related to how to configure the AWS EKS Cluster and its supported infrastructure resources provided by the module are supported, while cluster internal configuration is out of scope for this project</li> <li>The previous <code>bottlerocket</code> example has been removed in favor of demonstrating the use and configuration of Bottlerocket nodes via the respective <code>eks_managed_node_group</code> and <code>self_managed_node_group</code> examples</li> <li>The previous <code>launch_template</code> and <code>launch_templates_with_managed_node_groups</code> examples have been removed; only launch templates are now supported (default) and launch configuration support has been removed</li> <li>The previous <code>secrets_encryption</code> example has been removed; the functionality has been demonstrated in several of the new examples rendering this standalone example redundant</li> <li>The additional, custom IAM role policy for the cluster role has been removed. The permissions are either now provided in the attached managed AWS permission policies used or are no longer required</li> <li>The <code>kubeconfig.tpl</code> template; kubeconfig management is no longer supported under this module</li> <li>The HTTP Terraform provider (forked copy) dependency has been removed</li> </ul>"},{"location":"UPGRADE-18.0/#variable-and-output-changes","title":"Variable and output changes","text":"<ol> <li> <p>Removed variables:</p> <ul> <li><code>cluster_create_timeout</code>, <code>cluster_update_timeout</code>, and <code>cluster_delete_timeout</code> have been replaced with <code>cluster_timeouts</code></li> <li><code>kubeconfig_name</code></li> <li><code>kubeconfig_output_path</code></li> <li><code>kubeconfig_file_permission</code></li> <li><code>kubeconfig_api_version</code></li> <li><code>kubeconfig_aws_authenticator_command</code></li> <li><code>kubeconfig_aws_authenticator_command_args</code></li> <li><code>kubeconfig_aws_authenticator_additional_args</code></li> <li><code>kubeconfig_aws_authenticator_env_variables</code></li> <li><code>write_kubeconfig</code></li> <li><code>default_platform</code></li> <li><code>manage_aws_auth</code></li> <li><code>aws_auth_additional_labels</code></li> <li><code>map_accounts</code></li> <li><code>map_roles</code></li> <li><code>map_users</code></li> <li><code>fargate_subnets</code></li> <li><code>worker_groups_launch_template</code></li> <li><code>worker_security_group_id</code></li> <li><code>worker_ami_name_filter</code></li> <li><code>worker_ami_name_filter_windows</code></li> <li><code>worker_ami_owner_id</code></li> <li><code>worker_ami_owner_id_windows</code></li> <li><code>worker_additional_security_group_ids</code></li> <li><code>worker_sg_ingress_from_port</code></li> <li><code>workers_additional_policies</code></li> <li><code>worker_create_security_group</code></li> <li><code>worker_create_initial_lifecycle_hooks</code></li> <li><code>worker_create_cluster_primary_security_group_rules</code></li> <li><code>cluster_create_endpoint_private_access_sg_rule</code></li> <li><code>cluster_endpoint_private_access_cidrs</code></li> <li><code>cluster_endpoint_private_access_sg</code></li> <li><code>manage_worker_iam_resources</code></li> <li><code>workers_role_name</code></li> <li><code>attach_worker_cni_policy</code></li> <li><code>eks_oidc_root_ca_thumbprint</code></li> <li><code>create_fargate_pod_execution_role</code></li> <li><code>fargate_pod_execution_role_name</code></li> <li><code>cluster_egress_cidrs</code></li> <li><code>workers_egress_cidrs</code></li> <li><code>wait_for_cluster_timeout</code></li> <li>EKS Managed Node Group sub-module (was <code>node_groups</code>)</li> <li><code>default_iam_role_arn</code></li> <li><code>workers_group_defaults</code></li> <li><code>worker_security_group_id</code></li> <li><code>node_groups_defaults</code></li> <li><code>node_groups</code></li> <li><code>ebs_optimized_not_supported</code></li> <li>Fargate profile sub-module (was <code>fargate</code>)</li> <li><code>create_eks</code> and <code>create_fargate_pod_execution_role</code> have been replaced with simply <code>create</code></li> </ul> </li> <li> <p>Renamed variables:</p> <ul> <li><code>create_eks</code> -&gt; <code>create</code></li> <li><code>subnets</code> -&gt; <code>subnet_ids</code></li> <li><code>cluster_create_security_group</code> -&gt; <code>create_cluster_security_group</code></li> <li><code>cluster_log_retention_in_days</code> -&gt; <code>cloudwatch_log_group_retention_in_days</code></li> <li><code>cluster_log_kms_key_id</code> -&gt; <code>cloudwatch_log_group_kms_key_id</code></li> <li><code>manage_cluster_iam_resources</code> -&gt; <code>create_iam_role</code></li> <li><code>cluster_iam_role_name</code> -&gt; <code>iam_role_name</code></li> <li><code>permissions_boundary</code> -&gt; <code>iam_role_permissions_boundary</code></li> <li><code>iam_path</code> -&gt; <code>iam_role_path</code></li> <li><code>pre_userdata</code> -&gt; <code>pre_bootstrap_user_data</code></li> <li><code>additional_userdata</code> -&gt; <code>post_bootstrap_user_data</code></li> <li><code>worker_groups</code> -&gt; <code>self_managed_node_groups</code></li> <li><code>workers_group_defaults</code> -&gt; <code>self_managed_node_group_defaults</code></li> <li><code>node_groups</code> -&gt; <code>eks_managed_node_groups</code></li> <li><code>node_groups_defaults</code> -&gt; <code>eks_managed_node_group_defaults</code></li> <li>EKS Managed Node Group sub-module (was <code>node_groups</code>)</li> <li><code>create_eks</code> -&gt; <code>create</code></li> <li><code>worker_additional_security_group_ids</code> -&gt; <code>vpc_security_group_ids</code></li> <li>Fargate profile sub-module</li> <li><code>fargate_pod_execution_role_name</code> -&gt; <code>name</code></li> <li><code>create_fargate_pod_execution_role</code> -&gt; <code>create_iam_role</code></li> <li><code>subnets</code> -&gt; <code>subnet_ids</code></li> <li><code>iam_path</code> -&gt; <code>iam_role_path</code></li> <li><code>permissions_boundary</code> -&gt; <code>iam_role_permissions_boundary</code></li> </ul> </li> <li> <p>Added variables:</p> <ul> <li><code>cluster_additional_security_group_ids</code> added to allow users to add additional security groups to the cluster as needed</li> <li><code>cluster_security_group_name</code></li> <li><code>cluster_security_group_use_name_prefix</code> added to allow users to use either the name as specified or default to using the name specified as a prefix</li> <li><code>cluster_security_group_description</code></li> <li><code>cluster_security_group_additional_rules</code></li> <li><code>cluster_security_group_tags</code></li> <li><code>create_cloudwatch_log_group</code> added in place of the logic that checked if any cluster log types were enabled to allow users to opt in as they see fit</li> <li><code>create_node_security_group</code> added to create single security group that connects node groups and cluster in central location</li> <li><code>node_security_group_id</code></li> <li><code>node_security_group_name</code></li> <li><code>node_security_group_use_name_prefix</code></li> <li><code>node_security_group_description</code></li> <li><code>node_security_group_additional_rules</code></li> <li><code>node_security_group_tags</code></li> <li><code>iam_role_arn</code></li> <li><code>iam_role_use_name_prefix</code></li> <li><code>iam_role_description</code></li> <li><code>iam_role_additional_policies</code></li> <li><code>iam_role_tags</code></li> <li><code>cluster_addons</code></li> <li><code>cluster_identity_providers</code></li> <li><code>fargate_profile_defaults</code></li> <li><code>prefix_separator</code> added to support legacy behavior of not having a prefix separator</li> <li>EKS Managed Node Group sub-module (was <code>node_groups</code>)</li> <li><code>platform</code></li> <li><code>enable_bootstrap_user_data</code></li> <li><code>pre_bootstrap_user_data</code></li> <li><code>post_bootstrap_user_data</code></li> <li><code>bootstrap_extra_args</code></li> <li><code>user_data_template_path</code></li> <li><code>create_launch_template</code></li> <li><code>launch_template_name</code></li> <li><code>launch_template_use_name_prefix</code></li> <li><code>description</code></li> <li><code>ebs_optimized</code></li> <li><code>ami_id</code></li> <li><code>key_name</code></li> <li><code>launch_template_default_version</code></li> <li><code>update_launch_template_default_version</code></li> <li><code>disable_api_termination</code></li> <li><code>kernel_id</code></li> <li><code>ram_disk_id</code></li> <li><code>block_device_mappings</code></li> <li><code>capacity_reservation_specification</code></li> <li><code>cpu_options</code></li> <li><code>credit_specification</code></li> <li><code>elastic_gpu_specifications</code></li> <li><code>elastic_inference_accelerator</code></li> <li><code>enclave_options</code></li> <li><code>instance_market_options</code></li> <li><code>license_specifications</code></li> <li><code>metadata_options</code></li> <li><code>enable_monitoring</code></li> <li><code>network_interfaces</code></li> <li><code>placement</code></li> <li><code>min_size</code></li> <li><code>max_size</code></li> <li><code>desired_size</code></li> <li><code>use_name_prefix</code></li> <li><code>ami_type</code></li> <li><code>ami_release_version</code></li> <li><code>capacity_type</code></li> <li><code>disk_size</code></li> <li><code>force_update_version</code></li> <li><code>instance_types</code></li> <li><code>labels</code></li> <li><code>cluster_version</code></li> <li><code>launch_template_version</code></li> <li><code>remote_access</code></li> <li><code>taints</code></li> <li><code>update_config</code></li> <li><code>timeouts</code></li> <li><code>create_security_group</code></li> <li><code>security_group_name</code></li> <li><code>security_group_use_name_prefix</code></li> <li><code>security_group_description</code></li> <li><code>vpc_id</code></li> <li><code>security_group_rules</code></li> <li><code>cluster_security_group_id</code></li> <li><code>security_group_tags</code></li> <li><code>create_iam_role</code></li> <li><code>iam_role_arn</code></li> <li><code>iam_role_name</code></li> <li><code>iam_role_use_name_prefix</code></li> <li><code>iam_role_path</code></li> <li><code>iam_role_description</code></li> <li><code>iam_role_permissions_boundary</code></li> <li><code>iam_role_additional_policies</code></li> <li><code>iam_role_tags</code></li> <li>Fargate profile sub-module (was <code>fargate</code>)</li> <li><code>iam_role_arn</code> (for if <code>create_iam_role</code> is <code>false</code> to bring your own externally created role)</li> <li><code>iam_role_name</code></li> <li><code>iam_role_use_name_prefix</code></li> <li><code>iam_role_description</code></li> <li><code>iam_role_additional_policies</code></li> <li><code>iam_role_tags</code></li> <li><code>selectors</code></li> <li><code>timeouts</code></li> </ul> </li> <li> <p>Removed outputs:</p> <ul> <li><code>cluster_version</code></li> <li><code>kubeconfig</code></li> <li><code>kubeconfig_filename</code></li> <li><code>workers_asg_arns</code></li> <li><code>workers_asg_names</code></li> <li><code>workers_user_data</code></li> <li><code>workers_default_ami_id</code></li> <li><code>workers_default_ami_id_windows</code></li> <li><code>workers_launch_template_ids</code></li> <li><code>workers_launch_template_arns</code></li> <li><code>workers_launch_template_latest_versions</code></li> <li><code>worker_security_group_id</code></li> <li><code>worker_iam_instance_profile_arns</code></li> <li><code>worker_iam_instance_profile_names</code></li> <li><code>worker_iam_role_name</code></li> <li><code>worker_iam_role_arn</code></li> <li><code>fargate_profile_ids</code></li> <li><code>fargate_profile_arns</code></li> <li><code>fargate_iam_role_name</code></li> <li><code>fargate_iam_role_arn</code></li> <li><code>node_groups</code></li> <li><code>security_group_rule_cluster_https_worker_ingress</code></li> <li>EKS Managed Node Group sub-module (was <code>node_groups</code>)</li> <li><code>node_groups</code></li> <li><code>aws_auth_roles</code></li> <li>Fargate profile sub-module (was <code>fargate</code>)</li> <li><code>aws_auth_roles</code></li> </ul> </li> <li> <p>Renamed outputs:</p> <ul> <li><code>config_map_aws_auth</code> -&gt; <code>aws_auth_configmap_yaml</code></li> <li>Fargate profile sub-module (was <code>fargate</code>)</li> <li><code>fargate_profile_ids</code> -&gt; <code>fargate_profile_id</code></li> <li><code>fargate_profile_arns</code> -&gt; <code>fargate_profile_arn</code></li> </ul> </li> <li> <p>Added outputs:</p> <ul> <li><code>cluster_platform_version</code></li> <li><code>cluster_status</code></li> <li><code>cluster_security_group_arn</code></li> <li><code>cluster_security_group_id</code></li> <li><code>node_security_group_arn</code></li> <li><code>node_security_group_id</code></li> <li><code>cluster_iam_role_unique_id</code></li> <li><code>cluster_addons</code></li> <li><code>cluster_identity_providers</code></li> <li><code>fargate_profiles</code></li> <li><code>eks_managed_node_groups</code></li> <li><code>self_managed_node_groups</code></li> <li>EKS Managed Node Group sub-module (was <code>node_groups</code>)</li> <li><code>launch_template_id</code></li> <li><code>launch_template_arn</code></li> <li><code>launch_template_latest_version</code></li> <li><code>node_group_arn</code></li> <li><code>node_group_id</code></li> <li><code>node_group_resources</code></li> <li><code>node_group_status</code></li> <li><code>security_group_arn</code></li> <li><code>security_group_id</code></li> <li><code>iam_role_name</code></li> <li><code>iam_role_arn</code></li> <li><code>iam_role_unique_id</code></li> <li>Fargate profile sub-module (was <code>fargate</code>)</li> <li><code>iam_role_unique_id</code></li> <li><code>fargate_profile_status</code></li> </ul> </li> </ol>"},{"location":"UPGRADE-18.0/#upgrade-migrations","title":"Upgrade Migrations","text":""},{"location":"UPGRADE-18.0/#before-17x-example","title":"Before 17.x Example","text":"<pre><code>module \"eks\" {\n  source  = \"terraform-aws-modules/eks/aws\"\n  version = \"~&gt; 17.0\"\n\n  cluster_name                    = local.name\n  cluster_version                 = local.cluster_version\n  cluster_endpoint_private_access = true\n  cluster_endpoint_public_access  = true\n\n  vpc_id  = module.vpc.vpc_id\n  subnets = module.vpc.private_subnets\n\n  # Managed Node Groups\n  node_groups_defaults = {\n    ami_type  = \"AL2_x86_64\"\n    disk_size = 50\n  }\n\n  node_groups = {\n    node_group = {\n      min_capacity     = 1\n      max_capacity     = 10\n      desired_capacity = 1\n\n      instance_types = [\"t3.large\"]\n      capacity_type  = \"SPOT\"\n\n      update_config = {\n        max_unavailable_percentage = 50\n      }\n\n      k8s_labels = {\n        Environment = \"test\"\n        GithubRepo  = \"terraform-aws-eks\"\n        GithubOrg   = \"terraform-aws-modules\"\n      }\n\n      taints = [\n        {\n          key    = \"dedicated\"\n          value  = \"gpuGroup\"\n          effect = \"NO_SCHEDULE\"\n        }\n      ]\n\n      additional_tags = {\n        ExtraTag = \"example\"\n      }\n    }\n  }\n\n  # Worker groups\n  worker_additional_security_group_ids = [aws_security_group.additional.id]\n\n  worker_groups_launch_template = [\n    {\n      name                    = \"worker-group\"\n      override_instance_types = [\"m5.large\", \"m5a.large\", \"m5d.large\", \"m5ad.large\"]\n      spot_instance_pools     = 4\n      asg_max_size            = 5\n      asg_desired_capacity    = 2\n      kubelet_extra_args      = \"--node-labels=node.kubernetes.io/lifecycle=spot\"\n      public_ip               = true\n    },\n  ]\n\n  # Fargate\n  fargate_profiles = {\n    default = {\n      name = \"default\"\n      selectors = [\n        {\n          namespace = \"kube-system\"\n          labels = {\n            k8s-app = \"kube-dns\"\n          }\n        },\n        {\n          namespace = \"default\"\n        }\n      ]\n\n      tags = {\n        Owner = \"test\"\n      }\n\n      timeouts = {\n        create = \"20m\"\n        delete = \"20m\"\n      }\n    }\n  }\n\n  tags = {\n    Environment = \"test\"\n    GithubRepo  = \"terraform-aws-eks\"\n    GithubOrg   = \"terraform-aws-modules\"\n  }\n}\n</code></pre>"},{"location":"UPGRADE-18.0/#after-18x-example","title":"After 18.x Example","text":"<pre><code>module \"cluster_after\" {\n  source  = \"terraform-aws-modules/eks/aws\"\n  version = \"~&gt; 18.0\"\n\n  cluster_name                    = local.name\n  cluster_version                 = local.cluster_version\n  cluster_endpoint_private_access = true\n  cluster_endpoint_public_access  = true\n\n  vpc_id     = module.vpc.vpc_id\n  subnet_ids = module.vpc.private_subnets\n\n  eks_managed_node_group_defaults = {\n    ami_type  = \"AL2_x86_64\"\n    disk_size = 50\n  }\n\n  eks_managed_node_groups = {\n    node_group = {\n      min_size     = 1\n      max_size     = 10\n      desired_size = 1\n\n      instance_types = [\"t3.large\"]\n      capacity_type  = \"SPOT\"\n\n      update_config = {\n        max_unavailable_percentage = 50\n      }\n\n      labels = {\n        Environment = \"test\"\n        GithubRepo  = \"terraform-aws-eks\"\n        GithubOrg   = \"terraform-aws-modules\"\n      }\n\n      taints = [\n        {\n          key    = \"dedicated\"\n          value  = \"gpuGroup\"\n          effect = \"NO_SCHEDULE\"\n        }\n      ]\n\n      tags = {\n        ExtraTag = \"example\"\n      }\n    }\n  }\n\n  self_managed_node_group_defaults = {\n    vpc_security_group_ids = [aws_security_group.additional.id]\n  }\n\n  self_managed_node_groups = {\n    worker_group = {\n      name = \"worker-group\"\n\n      min_size      = 1\n      max_size      = 5\n      desired_size  = 2\n      instance_type = \"m4.large\"\n\n      bootstrap_extra_args = \"--kubelet-extra-args '--node-labels=node.kubernetes.io/lifecycle=spot'\"\n\n      block_device_mappings = {\n        xvda = {\n          device_name = \"/dev/xvda\"\n          ebs = {\n            delete_on_termination = true\n            encrypted             = false\n            volume_size           = 100\n            volume_type           = \"gp2\"\n          }\n\n        }\n      }\n\n      use_mixed_instances_policy = true\n      mixed_instances_policy = {\n        instances_distribution = {\n          spot_instance_pools = 4\n        }\n\n        override = [\n          { instance_type = \"m5.large\" },\n          { instance_type = \"m5a.large\" },\n          { instance_type = \"m5d.large\" },\n          { instance_type = \"m5ad.large\" },\n        ]\n      }\n    }\n  }\n\n  # Fargate\n  fargate_profiles = {\n    default = {\n      name = \"default\"\n\n      selectors = [\n        {\n          namespace = \"kube-system\"\n          labels = {\n            k8s-app = \"kube-dns\"\n          }\n        },\n        {\n          namespace = \"default\"\n        }\n      ]\n\n      tags = {\n        Owner = \"test\"\n      }\n\n      timeouts = {\n        create = \"20m\"\n        delete = \"20m\"\n      }\n    }\n  }\n\n  tags = {\n    Environment = \"test\"\n    GithubRepo  = \"terraform-aws-eks\"\n    GithubOrg   = \"terraform-aws-modules\"\n  }\n}\n</code></pre>"},{"location":"UPGRADE-18.0/#diff-of-before-after","title":"Diff of before &lt;&gt; after","text":"<pre><code> module \"eks\" {\n   source  = \"terraform-aws-modules/eks/aws\"\n-  version = \"~&gt; 17.0\"\n+  version = \"~&gt; 18.0\"\n\n   cluster_name                    = local.name\n   cluster_version                 = local.cluster_version\n   cluster_endpoint_private_access = true\n   cluster_endpoint_public_access  = true\n\n   vpc_id  = module.vpc.vpc_id\n-  subnets = module.vpc.private_subnets\n+  subnet_ids = module.vpc.private_subnets\n\n-  # Managed Node Groups\n-  node_groups_defaults = {\n+  eks_managed_node_group_defaults = {\n     ami_type  = \"AL2_x86_64\"\n     disk_size = 50\n   }\n\n-  node_groups = {\n+  eks_managed_node_groups = {\n     node_group = {\n-      min_capacity     = 1\n-      max_capacity     = 10\n-      desired_capacity = 1\n+      min_size     = 1\n+      max_size     = 10\n+      desired_size = 1\n\n       instance_types = [\"t3.large\"]\n       capacity_type  = \"SPOT\"\n\n       update_config = {\n         max_unavailable_percentage = 50\n       }\n\n-      k8s_labels = {\n+      labels = {\n         Environment = \"test\"\n         GithubRepo  = \"terraform-aws-eks\"\n         GithubOrg   = \"terraform-aws-modules\"\n       }\n\n       taints = [\n         {\n           key    = \"dedicated\"\n           value  = \"gpuGroup\"\n           effect = \"NO_SCHEDULE\"\n         }\n       ]\n\n-      additional_tags = {\n+      tags = {\n         ExtraTag = \"example\"\n       }\n     }\n   }\n\n-  # Worker groups\n-  worker_additional_security_group_ids = [aws_security_group.additional.id]\n-\n-  worker_groups_launch_template = [\n-    {\n-      name                    = \"worker-group\"\n-      override_instance_types = [\"m5.large\", \"m5a.large\", \"m5d.large\", \"m5ad.large\"]\n-      spot_instance_pools     = 4\n-      asg_max_size            = 5\n-      asg_desired_capacity    = 2\n-      kubelet_extra_args      = \"--node-labels=node.kubernetes.io/lifecycle=spot\"\n-      public_ip               = true\n-    },\n-  ]\n+  self_managed_node_group_defaults = {\n+    vpc_security_group_ids = [aws_security_group.additional.id]\n+  }\n+\n+  self_managed_node_groups = {\n+    worker_group = {\n+      name = \"worker-group\"\n+\n+      min_size      = 1\n+      max_size      = 5\n+      desired_size  = 2\n+      instance_type = \"m4.large\"\n+\n+      bootstrap_extra_args = \"--kubelet-extra-args '--node-labels=node.kubernetes.io/lifecycle=spot'\"\n+\n+      block_device_mappings = {\n+        xvda = {\n+          device_name = \"/dev/xvda\"\n+          ebs = {\n+            delete_on_termination = true\n+            encrypted             = false\n+            volume_size           = 100\n+            volume_type           = \"gp2\"\n+          }\n+\n+        }\n+      }\n+\n+      use_mixed_instances_policy = true\n+      mixed_instances_policy = {\n+        instances_distribution = {\n+          spot_instance_pools = 4\n+        }\n+\n+        override = [\n+          { instance_type = \"m5.large\" },\n+          { instance_type = \"m5a.large\" },\n+          { instance_type = \"m5d.large\" },\n+          { instance_type = \"m5ad.large\" },\n+        ]\n+      }\n+    }\n+  }\n\n   # Fargate\n   fargate_profiles = {\n     default = {\n       name = \"default\"\n       selectors = [\n         {\n           namespace = \"kube-system\"\n           labels = {\n             k8s-app = \"kube-dns\"\n           }\n         },\n         {\n           namespace = \"default\"\n         }\n       ]\n\n       tags = {\n         Owner = \"test\"\n       }\n\n       timeouts = {\n         create = \"20m\"\n         delete = \"20m\"\n       }\n     }\n   }\n\n   tags = {\n     Environment = \"test\"\n     GithubRepo  = \"terraform-aws-eks\"\n     GithubOrg   = \"terraform-aws-modules\"\n   }\n }\n</code></pre>"},{"location":"UPGRADE-18.0/#attaching-an-iam-role-policy-to-a-fargate-profile","title":"Attaching an IAM role policy to a Fargate profile","text":""},{"location":"UPGRADE-18.0/#before-17x","title":"Before 17.x","text":"<pre><code>resource \"aws_iam_role_policy_attachment\" \"default\" {\n  role       = module.eks.fargate_iam_role_name\n  policy_arn = aws_iam_policy.default.arn\n}\n</code></pre>"},{"location":"UPGRADE-18.0/#after-18x","title":"After 18.x","text":"<pre><code># Attach the policy to an \"example\" Fargate profile\nresource \"aws_iam_role_policy_attachment\" \"default\" {\n  role       = module.eks.fargate_profiles[\"example\"].iam_role_name\n  policy_arn = aws_iam_policy.default.arn\n}\n</code></pre> <p>Or:</p> <pre><code># Attach the policy to all Fargate profiles\nresource \"aws_iam_role_policy_attachment\" \"default\" {\n  for_each = module.eks.fargate_profiles\n\n  role       = each.value.iam_role_name\n  policy_arn = aws_iam_policy.default.arn\n}\n</code></pre>"},{"location":"UPGRADE-19.0/","title":"Upgrade from v18.x to v19.x","text":"<p>Please consult the <code>examples</code> directory for reference example configurations. If you find a bug, please open an issue with supporting configuration to reproduce.</p>"},{"location":"UPGRADE-19.0/#list-of-backwards-incompatible-changes","title":"List of backwards incompatible changes","text":"<ul> <li>The <code>cluster_id</code> output used to output the name of the cluster. This is due to the fact that the cluster name is a unique constraint and therefore its set as the unique identifier within Terraform's state map. However, starting with local EKS clusters created on Outposts, there is now an attribute returned from the <code>aws eks create-cluster</code> API named <code>id</code>. The <code>cluster_id</code> has been updated to return this value which means that for current, standard EKS clusters created in the AWS cloud, no value will be returned (at the time of this writing) for <code>cluster_id</code> and only local EKS clusters on Outposts will return a value that looks like a UUID/GUID. Users should switch all instances of <code>cluster_id</code> to use <code>cluster_name</code> before upgrading to v19. Reference</li> <li>Minimum supported version of Terraform AWS provider updated to v4.45 to support the latest features provided via the resources utilized.</li> <li>Minimum supported version of Terraform updated to v1.0</li> <li>Individual security group created per EKS managed node group or self-managed node group has been removed. This configuration went mostly unused and would often cause confusion (\"Why is there an empty security group attached to my nodes?\"). This functionality can easily be replicated by user's providing one or more externally created security groups to attach to nodes launched from the node group.</li> <li>Previously, <code>var.iam_role_additional_policies</code> (one for each of the following: cluster IAM role, EKS managed node group IAM role, self-managed node group IAM role, and Fargate Profile IAM role) accepted a list of strings. This worked well for policies that already existed but failed for policies being created at the same time as the cluster due to the well-known issue of unknown values used in a <code>for_each</code> loop. To rectify this issue in <code>v19.x</code>, two changes were made:</li> <li><code>var.iam_role_additional_policies</code> was changed from type <code>list(string)</code> to type <code>map(string)</code> -&gt; this is a breaking change. More information on managing this change can be found below, under <code>Terraform State Moves</code></li> <li>The logic used in the root module for this variable was changed to replace the use of <code>try()</code> with <code>lookup()</code>. More details on why can be found here</li> <li>The cluster name has been removed from the Karpenter module event rule names. Due to the use of long cluster names appending to the provided naming scheme, the cluster name has moved to a <code>ClusterName</code> tag and the event rule name is now a prefix. This guarantees that users can have multiple instances of Karpenter with their respective event rules/SQS queue without name collisions, while also still being able to identify which queues and event rules belong to which cluster.</li> <li>The new variable <code>node_security_group_enable_recommended_rules</code> is set to true by default and may conflict with any custom ingress/egress rules. Please ensure that any duplicates from the <code>node_security_group_additional_rules</code> are removed before upgrading, or set <code>node_security_group_enable_recommended_rules</code> to false. Reference</li> </ul>"},{"location":"UPGRADE-19.0/#additional-changes","title":"Additional changes","text":""},{"location":"UPGRADE-19.0/#added","title":"Added","text":"<ul> <li>Support for setting <code>preserve</code> as well as <code>most_recent</code> on addons.</li> <li><code>preserve</code> indicates if you want to preserve the created resources when deleting the EKS add-on</li> <li><code>most_recent</code> indicates if you want to use the most recent revision of the add-on or the default version (default)</li> <li>Support for setting default node security group rules for common access patterns required:</li> <li>Egress all for <code>0.0.0.0/0</code>/<code>::/0</code></li> <li>Ingress from cluster security group for 8443/TCP and 9443/TCP for common applications such as ALB Ingress Controller, Karpenter, OPA Gatekeeper, etc. These are commonly used as webhook ports for validating and mutating webhooks</li> </ul>"},{"location":"UPGRADE-19.0/#modified","title":"Modified","text":"<ul> <li><code>cluster_security_group_additional_rules</code> and <code>node_security_group_additional_rules</code> have been modified to use <code>lookup()</code> instead of <code>try()</code> to avoid the well-known issue of unknown values within a <code>for_each</code> loop</li> <li>Default cluster security group rules have removed egress rules for TCP/443 and TCP/10250 to node groups since the cluster primary security group includes a default rule for ALL to <code>0.0.0.0/0</code>/<code>::/0</code></li> <li>Default node security group rules have removed egress rules have been removed since the default security group settings have egress rule for ALL to <code>0.0.0.0/0</code>/<code>::/0</code></li> <li><code>block_device_mappings</code> previously required a map of maps but has since changed to an array of maps. Users can remove the outer key for each block device mapping and replace the outermost map <code>{}</code> with an array <code>[]</code>. There are no state changes required for this change.</li> <li><code>create_kms_key</code> previously defaulted to <code>false</code> and now defaults to <code>true</code>. Clusters created with this module now default to enabling secret encryption by default with a customer-managed KMS key created by this module</li> <li><code>cluster_encryption_config</code> previously used a type of <code>list(any)</code> and now uses a type of <code>any</code> -&gt; users can simply remove the outer <code>[</code>...<code>]</code> brackets on <code>v19.x</code></li> <li><code>cluster_encryption_config</code> previously defaulted to <code>[]</code> and now defaults to <code>{resources = [\"secrets\"]}</code> to encrypt secrets by default</li> <li><code>cluster_endpoint_public_access</code> previously defaulted to <code>true</code> and now defaults to <code>false</code>. Clusters created with this module now default to private-only access to the cluster endpoint</li> <li><code>cluster_endpoint_private_access</code> previously defaulted to <code>false</code> and now defaults to <code>true</code></li> <li>The addon configuration now sets <code>\"OVERWRITE\"</code> as the default value for <code>resolve_conflicts</code> to ease add-on upgrade management. Users can opt out of this by instead setting <code>\"NONE\"</code> as the value for <code>resolve_conflicts</code></li> <li>The <code>kms</code> module used has been updated from <code>v1.0.2</code> to <code>v1.1.0</code> - no material changes other than updated to latest</li> <li>The default value for EKS managed node group <code>update_config</code> has been updated to the recommended <code>{ max_unavailable_percentage = 33 }</code></li> <li>The default value for the self-managed node group <code>instance_refresh</code> has been updated to the recommended:     <pre><code>{\n  strategy = \"Rolling\"\n  preferences = {\n    min_healthy_percentage = 66\n  }\n}\n</code></pre></li> </ul>"},{"location":"UPGRADE-19.0/#removed","title":"Removed","text":"<ul> <li>Remove all references of <code>aws_default_tags</code> to avoid update conflicts; this is the responsibility of the provider and should be handled at the provider level</li> <li>https://github.com/terraform-aws-modules/terraform-aws-eks/issues?q=is%3Aissue+default_tags+is%3Aclosed</li> <li>https://github.com/terraform-aws-modules/terraform-aws-eks/pulls?q=is%3Apr+default_tags+is%3Aclosed</li> </ul>"},{"location":"UPGRADE-19.0/#variable-and-output-changes","title":"Variable and output changes","text":"<ol> <li> <p>Removed variables:</p> </li> <li> <p><code>node_security_group_ntp_ipv4_cidr_block</code> - default security group settings have an egress rule for ALL to <code>0.0.0.0/0</code>/<code>::/0</code></p> </li> <li><code>node_security_group_ntp_ipv6_cidr_block</code> - default security group settings have an egress rule for ALL to <code>0.0.0.0/0</code>/<code>::/0</code></li> <li>Self-managed node groups:<ul> <li><code>create_security_group</code></li> <li><code>security_group_name</code></li> <li><code>security_group_use_name_prefix</code></li> <li><code>security_group_description</code></li> <li><code>security_group_rules</code></li> <li><code>security_group_tags</code></li> <li><code>cluster_security_group_id</code></li> <li><code>vpc_id</code></li> </ul> </li> <li> <p>EKS managed node groups:</p> <ul> <li><code>create_security_group</code></li> <li><code>security_group_name</code></li> <li><code>security_group_use_name_prefix</code></li> <li><code>security_group_description</code></li> <li><code>security_group_rules</code></li> <li><code>security_group_tags</code></li> <li><code>cluster_security_group_id</code></li> <li><code>vpc_id</code></li> </ul> </li> <li> <p>Renamed variables:</p> </li> <li> <p>N/A</p> </li> <li> <p>Added variables:</p> </li> <li> <p><code>provision_on_outpost</code>for Outposts support</p> </li> <li><code>outpost_config</code> for Outposts support</li> <li><code>cluster_addons_timeouts</code> for setting a common set of timeouts for all addons (unless a specific value is provided within the addon configuration)</li> <li><code>service_ipv6_cidr</code> for setting the IPv6 CIDR block for the Kubernetes service addresses</li> <li> <p><code>node_security_group_enable_recommended_rules</code> for enabling recommended node security group rules for common access patterns</p> </li> <li> <p>Self-managed node groups:</p> <ul> <li><code>launch_template_id</code> for use when using an existing/externally created launch template (Ref: https://github.com/terraform-aws-modules/terraform-aws-autoscaling/pull/204)</li> <li><code>maintenance_options</code></li> <li><code>private_dns_name_options</code></li> <li><code>instance_requirements</code></li> <li><code>context</code></li> <li><code>default_instance_warmup</code></li> <li><code>force_delete_warm_pool</code></li> </ul> </li> <li>EKS managed node groups:<ul> <li><code>use_custom_launch_template</code> was added to better clarify how users can switch between a custom launch template or the default launch template provided by the EKS managed node group. Previously, to achieve this same functionality of using the default launch template, users needed to set <code>create_launch_template = false</code> and <code>launch_template_name = \"\"</code> which is not very intuitive.</li> <li><code>launch_template_id</code> for use when using an existing/externally created launch template (Ref: https://github.com/terraform-aws-modules/terraform-aws-autoscaling/pull/204)</li> <li><code>maintenance_options</code></li> <li><code>private_dns_name_options</code>  -</li> </ul> </li> <li> <p>Removed outputs:</p> </li> <li> <p>Self-managed node groups:</p> <ul> <li><code>security_group_arn</code></li> <li><code>security_group_id</code></li> </ul> </li> <li> <p>EKS managed node groups:</p> <ul> <li><code>security_group_arn</code></li> <li><code>security_group_id</code></li> </ul> </li> <li> <p>Renamed outputs:</p> </li> <li> <p><code>cluster_id</code> is not renamed but the value it returns is now different. For standard EKS clusters created in the AWS cloud, the value returned at the time of this writing is <code>null</code>/empty. For local EKS clusters created on Outposts, the value returned will look like a UUID/GUID. Users should switch all instances of <code>cluster_id</code> to use <code>cluster_name</code> before upgrading to v19. Reference</p> </li> <li> <p>Added outputs:</p> </li> <li> <p><code>cluster_name</code> - The <code>cluster_id</code> currently set by the AWS provider is actually the cluster name, but in the future, this will change and there will be a distinction between the <code>cluster_name</code> and <code>cluster_id</code>. Reference</p> </li> </ol>"},{"location":"UPGRADE-19.0/#upgrade-migrations","title":"Upgrade Migrations","text":"<ol> <li>Before upgrading your module definition to <code>v19.x</code>, please see below for both EKS managed node group(s) and self-managed node groups and remove the node group(s) security group prior to upgrading.</li> </ol>"},{"location":"UPGRADE-19.0/#self-managed-node-groups","title":"Self-Managed Node Groups","text":"<p>Self-managed node groups on <code>v18.x</code> by default create a security group that does not specify any rules. In <code>v19.x</code>, this security group has been removed due to the predominant lack of usage (most users rely on the shared node security group). While still using version <code>v18.x</code> of your module definition, remove this security group from your node groups by setting <code>create_security_group = false</code>.</p> <ul> <li>If you are currently utilizing this security group, it is recommended to create an additional security group that matches the rules/settings of the security group created by the node group, and specify that security group ID in <code>vpc_security_group_ids</code>. Once this is in place, you can proceed with the original security group removal.</li> <li>For most users, the security group is not used and can be safely removed. However, deployed instances will have the security group attached to nodes and require the security group to be disassociated before the security group can be deleted. Because instances are deployed via autoscaling groups, we cannot simply remove the security group from the code and have those changes reflected on the instances. Instead, we have to update the code and then trigger the autoscaling groups to cycle the instances deployed so that new instances are provisioned without the security group attached. You can utilize the <code>instance_refresh</code> parameter of Autoscaling groups to force nodes to re-deploy when removing the security group since changes to launch templates automatically trigger an instance refresh. An example configuration is provided below.</li> <li>Add the following to either/or <code>self_managed_node_group_defaults</code> or the individual self-managed node group definitions:     <pre><code>create_security_group = false\ninstance_refresh = {\n  strategy = \"Rolling\"\n  preferences = {\n    min_healthy_percentage = 66\n  }\n}\n</code></pre></li> <li>It is recommended to use the <code>aws-node-termination-handler</code> while performing this update. Please refer to the <code>irsa-autoscale-refresh</code> example for usage. This will ensure that pods are safely evicted in a controlled manner to avoid service disruptions.</li> <li>Once the necessary configurations are in place, you can apply the changes which will:</li> <li>Create a new launch template (version) without the self-managed node group security group</li> <li>Replace instances based on the <code>instance_refresh</code> configuration settings</li> <li>New instances will launch without the self-managed node group security group, and prior instances will be terminated</li> <li>Once the self-managed node group has cycled, the security group will be deleted</li> </ul>"},{"location":"UPGRADE-19.0/#eks-managed-node-groups","title":"EKS Managed Node Groups","text":"<p>EKS managed node groups on <code>v18.x</code> by default create a security group that does not specify any rules. In <code>v19.x</code>, this security group has been removed due to the predominant lack of usage (most users rely on the shared node security group). While still using version <code>v18.x</code> of your module definition, remove this security group from your node groups by setting <code>create_security_group = false</code>.</p> <ul> <li>If you are currently utilizing this security group, it is recommended to create an additional security group that matches the rules/settings of the security group created by the node group, and specify that security group ID in <code>vpc_security_group_ids</code>. Once this is in place, you can proceed with the original security group removal.</li> <li>EKS managed node groups rollout changes using a rolling update strategy that can be influenced through <code>update_config</code>. No additional changes are required for removing the security group created by node groups (unlike self-managed node groups which should utilize the <code>instance_refresh</code> setting of Autoscaling groups).</li> <li>Once <code>create_security_group = false</code> has been set, you can apply the changes which will:</li> <li>Create a new launch template (version) without the EKS managed node group security group</li> <li>Replace instances based on the <code>update_config</code> configuration settings</li> <li>New instances will launch without the EKS managed node group security group, and prior instances will be terminated</li> <li> <p>Once the EKS managed node group has cycled, the security group will be deleted</p> </li> <li> <p>Once the node group security group(s) have been removed, you can update your module definition to specify the <code>v19.x</code> version of the module</p> </li> <li>Run <code>terraform init -upgrade=true</code> to update your configuration and pull in the v19 changes</li> <li>Using the documentation provided above, update your module definition to reflect the changes in the module from <code>v18.x</code> to <code>v19.x</code>. You can utilize <code>terraform plan</code> as you go to help highlight any changes that you wish to make. See below for <code>terraform state mv ...</code> commands related to the use of <code>iam_role_additional_policies</code>. If you are not providing any values to these variables, you can skip this section.</li> <li>Once you are satisfied with the changes and the <code>terraform plan</code> output, you can apply the changes to sync your infrastructure with the updated module definition (or vice versa).</li> </ul>"},{"location":"UPGRADE-19.0/#diff-of-before-v18x-vs-after-v19x","title":"Diff of Before (v18.x) vs After (v19.x)","text":"<pre><code> module \"eks\" {\n   source  = \"terraform-aws-modules/eks/aws\"\n-  version = \"~&gt; 18.0\"\n+  version = \"~&gt; 19.0\"\n\n  cluster_name                    = local.name\n+ cluster_endpoint_public_access  = true\n- cluster_endpoint_private_access = true # now the default\n\n  cluster_addons = {\n-   resolve_conflicts = \"OVERWRITE\" # now the default\n+   preserve          = true\n+   most_recent       = true\n\n+   timeouts = {\n+     create = \"25m\"\n+     delete = \"10m\"\n    }\n    kube-proxy = {}\n    vpc-cni = {\n-     resolve_conflicts = \"OVERWRITE\" # now the default\n    }\n  }\n\n  # Encryption key\n  create_kms_key = true\n- cluster_encryption_config = [{\n-   resources = [\"secrets\"]\n- }]\n+ cluster_encryption_config = {\n+   resources = [\"secrets\"]\n+ }\n  kms_key_deletion_window_in_days = 7\n  enable_kms_key_rotation         = true\n\n- iam_role_additional_policies = [aws_iam_policy.additional.arn]\n+ iam_role_additional_policies = {\n+   additional = aws_iam_policy.additional.arn\n+ }\n\n  vpc_id                   = module.vpc.vpc_id\n  subnet_ids               = module.vpc.private_subnets\n  control_plane_subnet_ids = module.vpc.intra_subnets\n\n  # Extend node-to-node security group rules\n- node_security_group_ntp_ipv4_cidr_block = [\"169.254.169.123/32\"] # now the default\n  node_security_group_additional_rules = {\n-    ingress_self_ephemeral = {\n-      description = \"Node to node ephemeral ports\"\n-      protocol    = \"tcp\"\n-      from_port   = 0\n-      to_port     = 0\n-      type        = \"ingress\"\n-      self        = true\n-    }\n-    egress_all = {\n-      description      = \"Node all egress\"\n-      protocol         = \"-1\"\n-      from_port        = 0\n-      to_port          = 0\n-      type             = \"egress\"\n-      cidr_blocks      = [\"0.0.0.0/0\"]\n-      ipv6_cidr_blocks = [\"::/0\"]\n-    }\n  }\n\n  # Self-Managed Node Group(s)\n  self_managed_node_group_defaults = {\n    vpc_security_group_ids = [aws_security_group.additional.id]\n-   iam_role_additional_policies = [aws_iam_policy.additional.arn]\n+   iam_role_additional_policies = {\n+     additional = aws_iam_policy.additional.arn\n+   }\n  }\n\n  self_managed_node_groups = {\n    spot = {\n      instance_type = \"m5.large\"\n      instance_market_options = {\n        market_type = \"spot\"\n      }\n\n      pre_bootstrap_user_data = &lt;&lt;-EOT\n        echo \"foo\"\n        export FOO=bar\n      EOT\n\n      bootstrap_extra_args = \"--kubelet-extra-args '--node-labels=node.kubernetes.io/lifecycle=spot'\"\n\n      post_bootstrap_user_data = &lt;&lt;-EOT\n        cd /tmp\n        sudo yum install -y https://s3.amazonaws.com/ec2-downloads-windows/SSMAgent/latest/linux_amd64/amazon-ssm-agent.rpm\n        sudo systemctl enable amazon-ssm-agent\n        sudo systemctl start amazon-ssm-agent\n      EOT\n\n-     create_security_group          = true\n-     security_group_name            = \"eks-managed-node-group-complete-example\"\n-     security_group_use_name_prefix = false\n-     security_group_description     = \"EKS managed node group complete example security group\"\n-     security_group_rules = {}\n-     security_group_tags = {}\n    }\n  }\n\n  # EKS Managed Node Group(s)\n  eks_managed_node_group_defaults = {\n    ami_type       = \"AL2_x86_64\"\n    instance_types = [\"m6i.large\", \"m5.large\", \"m5n.large\", \"m5zn.large\"]\n\n    attach_cluster_primary_security_group = true\n    vpc_security_group_ids                = [aws_security_group.additional.id]\n-   iam_role_additional_policies = [aws_iam_policy.additional.arn]\n+   iam_role_additional_policies = {\n+     additional = aws_iam_policy.additional.arn\n+   }\n  }\n\n  eks_managed_node_groups = {\n    blue = {}\n    green = {\n      min_size     = 1\n      max_size     = 10\n      desired_size = 1\n\n      instance_types = [\"t3.large\"]\n      capacity_type  = \"SPOT\"\n      labels = {\n        Environment = \"test\"\n        GithubRepo  = \"terraform-aws-eks\"\n        GithubOrg   = \"terraform-aws-modules\"\n      }\n\n      taints = {\n        dedicated = {\n          key    = \"dedicated\"\n          value  = \"gpuGroup\"\n          effect = \"NO_SCHEDULE\"\n        }\n      }\n\n      update_config = {\n        max_unavailable_percentage = 33 # or set `max_unavailable`\n      }\n\n-     create_security_group          = true\n-     security_group_name            = \"eks-managed-node-group-complete-example\"\n-     security_group_use_name_prefix = false\n-     security_group_description     = \"EKS managed node group complete example security group\"\n-     security_group_rules = {}\n-     security_group_tags = {}\n\n      tags = {\n        ExtraTag = \"example\"\n      }\n    }\n  }\n\n  # Fargate Profile(s)\n  fargate_profile_defaults = {\n-   iam_role_additional_policies = [aws_iam_policy.additional.arn]\n+   iam_role_additional_policies = {\n+     additional = aws_iam_policy.additional.arn\n+   }\n  }\n\n  fargate_profiles = {\n    default = {\n      name = \"default\"\n      selectors = [\n        {\n          namespace = \"kube-system\"\n          labels = {\n            k8s-app = \"kube-dns\"\n          }\n        },\n        {\n          namespace = \"default\"\n        }\n      ]\n\n      tags = {\n        Owner = \"test\"\n      }\n\n      timeouts = {\n        create = \"20m\"\n        delete = \"20m\"\n      }\n    }\n  }\n\n  # OIDC Identity provider\n  cluster_identity_providers = {\n    cognito = {\n      client_id      = \"702vqsrjicklgb7c5b7b50i1gc\"\n      issuer_url     = \"https://cognito-idp.us-west-2.amazonaws.com/us-west-2_re1u6bpRA\"\n      username_claim = \"email\"\n      groups_claim   = \"cognito:groups\"\n      groups_prefix  = \"gid:\"\n    }\n  }\n\n  # aws-auth configmap\n  manage_aws_auth_configmap = true\n\n  aws_auth_node_iam_role_arns_non_windows = [\n    module.eks_managed_node_group.iam_role_arn,\n    module.self_managed_node_group.iam_role_arn,\n  ]\n  aws_auth_fargate_profile_pod_execution_role_arns = [\n    module.fargate_profile.fargate_profile_pod_execution_role_arn\n  ]\n\n  aws_auth_roles = [\n    {\n      rolearn  = \"arn:aws:iam::66666666666:role/role1\"\n      username = \"role1\"\n      groups   = [\"system:masters\"]\n    },\n  ]\n\n  aws_auth_users = [\n    {\n      userarn  = \"arn:aws:iam::66666666666:user/user1\"\n      username = \"user1\"\n      groups   = [\"system:masters\"]\n    },\n    {\n      userarn  = \"arn:aws:iam::66666666666:user/user2\"\n      username = \"user2\"\n      groups   = [\"system:masters\"]\n    },\n  ]\n\n  aws_auth_accounts = [\n    \"777777777777\",\n    \"888888888888\",\n  ]\n\n  tags = local.tags\n}\n</code></pre>"},{"location":"UPGRADE-19.0/#terraform-state-moves","title":"Terraform State Moves","text":"<p>The following Terraform state move commands are optional but recommended if you are providing additional IAM policies that are to be attached to IAM roles created by this module (cluster IAM role, node group IAM role, Fargate profile IAM role). Because the resources affected are <code>aws_iam_role_policy_attachment</code>, in theory, you could get away with simply applying the configuration and letting Terraform detach and re-attach the policies. However, during this brief period of update, you could experience permission failures as the policy is detached and re-attached, and therefore the state move route is recommended.</p> <p>Where <code>\"&lt;POLICY_ARN&gt;\"</code> is specified, this should be replaced with the full ARN of the policy, and <code>\"&lt;POLICY_MAP_KEY&gt;\"</code> should be replaced with the key used in the <code>iam_role_additional_policies</code> map for the associated policy. For example, if you have the following<code>v19.x</code> configuration:</p> <pre><code>  ...\n  # This is demonstrating the cluster IAM role additional policies\n  iam_role_additional_policies = {\n    additional = aws_iam_policy.additional.arn\n  }\n  ...\n</code></pre> <p>The associated state move command would look similar to (albeit with your correct policy ARN):</p> <pre><code>terraform state mv 'module.eks.aws_iam_role_policy_attachment.this[\"arn:aws:iam::111111111111:policy/ex-complete-additional\"]' 'module.eks.aws_iam_role_policy_attachment.additional[\"additional\"]'\n</code></pre> <p>If you are not providing any additional IAM policies, no actions are required.</p>"},{"location":"UPGRADE-19.0/#cluster-iam-role","title":"Cluster IAM Role","text":"<p>Repeat for each policy provided in <code>iam_role_additional_policies</code>:</p> <pre><code>terraform state mv 'module.eks.aws_iam_role_policy_attachment.this[\"&lt;POLICY_ARN&gt;\"]' 'module.eks.aws_iam_role_policy_attachment.additional[\"&lt;POLICY_MAP_KEY&gt;\"]'\n</code></pre>"},{"location":"UPGRADE-19.0/#eks-managed-node-group-iam-role","title":"EKS Managed Node Group IAM Role","text":"<p>Where <code>\"&lt;NODE_GROUP_KEY&gt;\"</code> is the key used in the <code>eks_managed_node_groups</code> map for the associated node group. Repeat for each policy provided in <code>iam_role_additional_policies</code> in either/or <code>eks_managed_node_group_defaults</code> or the individual node group definitions:</p> <pre><code>terraform state mv 'module.eks.module.eks_managed_node_group[\"&lt;NODE_GROUP_KEY&gt;\"].aws_iam_role_policy_attachment.this[\"&lt;POLICY_ARN&gt;\"]' 'module.eks.module.eks_managed_node_group[\"&lt;NODE_GROUP_KEY&gt;\"].aws_iam_role_policy_attachment.additional[\"&lt;POLICY_MAP_KEY&gt;\"]'\n</code></pre>"},{"location":"UPGRADE-19.0/#self-managed-node-group-iam-role","title":"Self-Managed Node Group IAM Role","text":"<p>Where <code>\"&lt;NODE_GROUP_KEY&gt;\"</code> is the key used in the <code>self_managed_node_groups</code> map for the associated node group. Repeat for each policy provided in <code>iam_role_additional_policies</code> in either/or <code>self_managed_node_group_defaults</code> or the individual node group definitions:</p> <pre><code>terraform state mv 'module.eks.module.self_managed_node_group[\"&lt;NODE_GROUP_KEY&gt;\"].aws_iam_role_policy_attachment.this[\"&lt;POLICY_ARN&gt;\"]' 'module.eks.module.self_managed_node_group[\"&lt;NODE_GROUP_KEY&gt;\"].aws_iam_role_policy_attachment.additional[\"&lt;POLICY_MAP_KEY&gt;\"]'\n</code></pre>"},{"location":"UPGRADE-19.0/#fargate-profile-iam-role","title":"Fargate Profile IAM Role","text":"<p>Where <code>\"&lt;FARGATE_PROFILE_KEY&gt;\"</code> is the key used in the <code>fargate_profiles</code> map for the associated profile. Repeat for each policy provided in <code>iam_role_additional_policies</code> in either/or <code>fargate_profile_defaults</code> or the individual profile definitions:</p> <pre><code>terraform state mv 'module.eks.module.fargate_profile[\"&lt;FARGATE_PROFILE_KEY&gt;\"].aws_iam_role_policy_attachment.this[\"&lt;POLICY_ARN&gt;\"]' 'module.eks.module.fargate_profile[\"&lt;FARGATE_PROFILE_KEY&gt;\"].aws_iam_role_policy_attachment.additional[\"&lt;POLICY_MAP_KEY&gt;\"]'\n</code></pre>"},{"location":"UPGRADE-20.0/","title":"Upgrade from v19.x to v20.x","text":"<p>Please consult the <code>examples</code> directory for reference example configurations. If you find a bug, please open an issue with supporting configuration to reproduce.</p>"},{"location":"UPGRADE-20.0/#list-of-backwards-incompatible-changes","title":"List of backwards incompatible changes","text":"<ul> <li>Minium supported AWS provider version increased to <code>v5.34</code></li> <li>Minimum supported Terraform version increased to <code>v1.3</code> to support Terraform state <code>moved</code> blocks as well as other advanced features</li> <li>The <code>resolve_conflicts</code> argument within the <code>cluster_addons</code> configuration has been replaced with <code>resolve_conflicts_on_create</code> and <code>resolve_conflicts_on_update</code> now that <code>resolve_conflicts</code> is deprecated</li> <li>The default/fallback value for the <code>preserve</code> argument of <code>cluster_addons</code>is now set to <code>true</code>. This has shown to be useful for users deprovisioning clusters while avoiding the situation where the CNI is deleted too early and causes resources to be left orphaned resulting in conflicts.</li> <li>The Karpenter sub-module's use of the <code>irsa</code> naming convention has been removed, along with an update to the Karpenter controller IAM policy to align with Karpenter's <code>v1beta1</code>/<code>v0.32</code> changes. Instead of referring to the role as <code>irsa</code> or <code>pod_identity</code>, its simply just an IAM role used by the Karpenter controller and there is support for use with either IRSA and/or Pod Identity (default) at this time</li> <li>The <code>aws-auth</code> ConfigMap resources have been moved to a standalone sub-module. This removes the Kubernetes provider requirement from the main module and allows for the <code>aws-auth</code> ConfigMap to be managed independently of the main module. This sub-module will be removed entirely in the next major release.</li> <li>Support for cluster access management has been added with the default authentication mode set as <code>API_AND_CONFIG_MAP</code>. Support for <code>CONFIG_MAP</code> is no longer supported; instead you will need to use <code>API_AND_CONFIG_MAP</code> at minimum</li> <li>Karpenter EventBridge rule key <code>spot_interrupt</code> updated to correct mis-spelling (was <code>spot_interupt</code>). This will cause the rule to be replaced</li> </ul>"},{"location":"UPGRADE-20.0/#upcoming-changes-planned-in-v210","title":"\u26a0\ufe0f Upcoming Changes Planned in v21.0 \u26a0\ufe0f","text":"<p>To give users advanced notice and provide some future direction for this module, these are the following changes we will be looking to make in the next major release of this module:</p> <ol> <li>The <code>aws-auth</code> sub-module will be removed entirely from the project. Since this sub-module is captured in the v20.x releases, users can continue using it even after the module moves forward with the next major version. The long term strategy and direction is cluster access entry and to rely only on the AWS Terraform provider.</li> <li>The default value for <code>authentication_mode</code> will change to <code>API</code>. Aligning with point 1 above, this is a one way change, but users are free to specify the value of their choosing in place of this default (when the change is made). This module will proceed with an EKS API first strategy.</li> <li>The launch template and autoscaling group usage contained within the EKS managed node group and self-managed node group sub-modules *might be replaced with the <code>terraform-aws-autoscaling</code> module. At minimum, it makes sense to replace most of functionality in the self-managed node group module with this external module, but its not yet clear if there is any benefit of using it in the EKS managed node group sub-module. The interface that users interact with will stay the same, the changes will be internal to the implementation and we will do everything we can to keep the disruption to a minimum.</li> <li>The <code>platform</code> variable will be replaced and instead <code>ami_type</code> will become the standard across both self-managed node group(s) and EKS managed node group(s). As EKS expands its portfolio of supported operating systems, the <code>ami_type</code> is better suited to associate the correct user data format to the respective OS. The <code>platform</code> variable is a legacy artifact of self-managed node groups but not as descriptive as the <code>ami_type</code>, and therefore it will be removed in favor of <code>ami_type</code>.</li> </ol>"},{"location":"UPGRADE-20.0/#additional-changes","title":"Additional changes","text":""},{"location":"UPGRADE-20.0/#added","title":"Added","text":"<ul> <li>A module tag has been added to the cluster control plane</li> <li>Support for cluster access entries. The <code>bootstrap_cluster_creator_admin_permissions</code> setting on the control plane has been hardcoded to <code>false</code> since this operation is a one time operation only at cluster creation per the EKS API. Instead, users can enable/disable <code>enable_cluster_creator_admin_permissions</code> at any time to achieve the same functionality. This takes the identity that Terraform is using to make API calls and maps it into a cluster admin via an access entry. For users on existing clusters, you will need to remove the default cluster administrator that was created by EKS prior to the cluster access entry APIs - see the section <code>Removing the default cluster administrator</code> for more details.</li> <li>Support for specifying the CloudWatch log group class (standard or infrequent access)</li> <li>Native support for Windows based managed node groups similar to AL2 and Bottlerocket</li> <li>Self-managed node groups now support <code>instance_maintenance_policy</code> and have added <code>max_healthy_percentage</code>, <code>scale_in_protected_instances</code>, and <code>standby_instances</code> arguments to the <code>instance_refresh.preferences</code> block</li> </ul>"},{"location":"UPGRADE-20.0/#modified","title":"Modified","text":"<ul> <li>For <code>sts:AssumeRole</code> permissions by services, the use of dynamically looking up the DNS suffix has been replaced with the static value of <code>amazonaws.com</code>. This does not appear to change by partition and instead requires users to set this manually for non-commercial regions.</li> <li>The default value for <code>kms_key_enable_default_policy</code> has changed from <code>false</code> to <code>true</code> to align with the default behavior of the <code>aws_kms_key</code> resource</li> <li>The Karpenter default value for <code>create_instance_profile</code> has changed from <code>true</code> to <code>false</code> to align with the changes in Karpenter v0.32</li> <li>The Karpenter variable <code>create_instance_profile</code> default value has changed from <code>true</code> to <code>false</code>. Starting with Karpenter <code>v0.32.0</code>, Karpenter accepts an IAM role and creates the EC2 instance profile used by the nodes</li> </ul>"},{"location":"UPGRADE-20.0/#removed","title":"Removed","text":"<ul> <li>The <code>complete</code> example has been removed due to its redundancy with the other examples</li> <li>References to the IRSA sub-module in the IAM repository have been removed. Once https://github.com/clowdhaus/terraform-aws-eks-pod-identity has been updated and moved into the organization, the documentation here will be updated to mention the new module.</li> </ul>"},{"location":"UPGRADE-20.0/#variable-and-output-changes","title":"Variable and output changes","text":"<ol> <li> <p>Removed variables:</p> </li> <li> <p><code>cluster_iam_role_dns_suffix</code> - replaced with a static string of <code>amazonaws.com</code></p> </li> <li><code>manage_aws_auth_configmap</code></li> <li><code>create_aws_auth_configmap</code></li> <li><code>aws_auth_node_iam_role_arns_non_windows</code></li> <li><code>aws_auth_node_iam_role_arns_windows</code></li> <li><code>aws_auth_fargate_profile_pod_execution_role_arn</code></li> <li><code>aws_auth_roles</code></li> <li><code>aws_auth_users</code></li> <li> <p><code>aws_auth_accounts</code></p> </li> <li> <p>Karpenter</p> <ul> <li><code>irsa_tag_key</code></li> <li><code>irsa_tag_values</code></li> <li><code>irsa_subnet_account_id</code></li> <li><code>enable_karpenter_instance_profile_creation</code></li> </ul> </li> <li> <p>Renamed variables:</p> </li> <li> <p>Karpenter</p> <ul> <li><code>create_irsa</code> -&gt; <code>create_iam_role</code></li> <li><code>irsa_name</code> -&gt; <code>iam_role_name</code></li> <li><code>irsa_use_name_prefix</code> -&gt; <code>iam_role_name_prefix</code></li> <li><code>irsa_path</code> -&gt; <code>iam_role_path</code></li> <li><code>irsa_description</code> -&gt; <code>iam_role_description</code></li> <li><code>irsa_max_session_duration</code> -&gt; <code>iam_role_max_session_duration</code></li> <li><code>irsa_permissions_boundary_arn</code> -&gt; <code>iam_role_permissions_boundary_arn</code></li> <li><code>irsa_tags</code> -&gt; <code>iam_role_tags</code></li> <li><code>policies</code> -&gt; <code>iam_role_policies</code></li> <li><code>irsa_policy_name</code> -&gt; <code>iam_policy_name</code></li> <li><code>irsa_ssm_parameter_arns</code> -&gt; <code>ami_id_ssm_parameter_arns</code></li> <li><code>create_iam_role</code> -&gt; <code>create_node_iam_role</code></li> <li><code>iam_role_additional_policies</code> -&gt; <code>node_iam_role_additional_policies</code></li> <li><code>policies</code> -&gt; <code>iam_role_policies</code></li> <li><code>iam_role_arn</code> -&gt; <code>node_iam_role_arn</code></li> <li><code>iam_role_name</code> -&gt; <code>node_iam_role_name</code></li> <li><code>iam_role_name_prefix</code> -&gt; <code>node_iam_role_name_prefix</code></li> <li><code>iam_role_path</code> -&gt; <code>node_iam_role_path</code></li> <li><code>iam_role_description</code> -&gt; <code>node_iam_role_description</code></li> <li><code>iam_role_max_session_duration</code> -&gt; <code>node_iam_role_max_session_duration</code></li> <li><code>iam_role_permissions_boundary_arn</code> -&gt; <code>node_iam_role_permissions_boundary_arn</code></li> <li><code>iam_role_attach_cni_policy</code> -&gt; <code>node_iam_role_attach_cni_policy</code></li> <li><code>iam_role_additional_policies</code> -&gt; <code>node_iam_role_additional_policies</code></li> <li><code>iam_role_tags</code> -&gt; <code>node_iam_role_tags</code></li> </ul> </li> <li> <p>Added variables:</p> </li> <li> <p><code>create_access_entry</code></p> </li> <li><code>enable_cluster_creator_admin_permissions</code></li> <li><code>authentication_mode</code></li> <li><code>access_entries</code></li> <li> <p><code>cloudwatch_log_group_class</code></p> </li> <li> <p>Karpenter</p> <ul> <li><code>iam_policy_name</code></li> <li><code>iam_policy_use_name_prefix</code></li> <li><code>iam_policy_description</code></li> <li><code>iam_policy_path</code></li> <li><code>enable_irsa</code></li> <li><code>create_access_entry</code></li> <li><code>access_entry_type</code></li> </ul> </li> <li> <p>Self-managed node group</p> <ul> <li><code>instance_maintenance_policy</code></li> <li><code>create_access_entry</code></li> <li><code>iam_role_arn</code></li> </ul> </li> <li> <p>Removed outputs:</p> </li> <li> <p><code>aws_auth_configmap_yaml</code></p> </li> <li> <p>Renamed outputs:</p> </li> <li> <p>Karpenter</p> <ul> <li><code>irsa_name</code> -&gt; <code>iam_role_name</code></li> <li><code>irsa_arn</code> -&gt; <code>iam_role_arn</code></li> <li><code>irsa_unique_id</code> -&gt; <code>iam_role_unique_id</code></li> <li><code>role_name</code> -&gt; <code>node_iam_role_name</code></li> <li><code>role_arn</code> -&gt; <code>node_iam_role_arn</code></li> <li><code>role_unique_id</code> -&gt; <code>node_iam_role_unique_id</code></li> </ul> </li> <li> <p>Added outputs:</p> </li> <li> <p><code>access_entries</code></p> </li> <li> <p>Karpenter</p> <ul> <li><code>node_access_entry_arn</code></li> </ul> </li> <li> <p>Self-managed node group</p> <ul> <li><code>access_entry_arn</code></li> </ul> </li> </ol>"},{"location":"UPGRADE-20.0/#upgrade-migrations","title":"Upgrade Migrations","text":""},{"location":"UPGRADE-20.0/#diff-of-before-v1921-vs-after-v200","title":"Diff of Before (v19.21) vs After (v20.0)","text":"<pre><code> module \"eks\" {\n   source  = \"terraform-aws-modules/eks/aws\"\n-  version = \"~&gt; 19.21\"\n+  version = \"~&gt; 20.0\"\n\n# If you want to maintain the current default behavior of v19.x\n+  kms_key_enable_default_policy = false\n\n-   manage_aws_auth_configmap = true\n\n-   aws_auth_roles = [\n-     {\n-       rolearn  = \"arn:aws:iam::66666666666:role/role1\"\n-       username = \"role1\"\n-       groups   = [\"custom-role-group\"]\n-     },\n-   ]\n\n-   aws_auth_users = [\n-     {\n-       userarn  = \"arn:aws:iam::66666666666:user/user1\"\n-       username = \"user1\"\n-       groups   = [\"custom-users-group\"]\n-     },\n-   ]\n}\n\n+ module \"eks_aws_auth\" {\n+   source  = \"terraform-aws-modules/eks/aws//modules/aws-auth\"\n+   version = \"~&gt; 20.0\"\n\n+   manage_aws_auth_configmap = true\n\n+   aws_auth_roles = [\n+     {\n+       rolearn  = \"arn:aws:iam::66666666666:role/role1\"\n+       username = \"role1\"\n+       groups   = [\"custom-role-group\"]\n+     },\n+   ]\n\n+   aws_auth_users = [\n+     {\n+       userarn  = \"arn:aws:iam::66666666666:user/user1\"\n+       username = \"user1\"\n+       groups   = [\"custom-users-group\"]\n+     },\n+   ]\n+ }\n</code></pre>"},{"location":"UPGRADE-20.0/#karpenter-diff-of-before-v1921-vs-after-v200","title":"Karpenter Diff of Before (v19.21) vs After (v20.0)","text":"<pre><code> module \"eks_karpenter\" {\n   source  = \"terraform-aws-modules/eks/aws//modules/karpenter\"\n-  version = \"~&gt; 19.21\"\n+  version = \"~&gt; 20.0\"\n\n# If you wish to maintain the current default behavior of v19.x\n+  enable_irsa             = true\n+  create_instance_profile = true\n\n# To avoid any resource re-creation\n+  iam_role_name          = \"KarpenterIRSA-${module.eks.cluster_name}\"\n+  iam_role_description   = \"Karpenter IAM role for service account\"\n+  iam_policy_name        = \"KarpenterIRSA-${module.eks.cluster_name}\"\n+  iam_policy_description = \"Karpenter IAM role for service account\"\n}\n</code></pre>"},{"location":"UPGRADE-20.0/#terraform-state-moves","title":"Terraform State Moves","text":""},{"location":"UPGRADE-20.0/#authentication-mode-changes","title":"\u26a0\ufe0f Authentication Mode Changes \u26a0\ufe0f","text":"<p>Changing the <code>authentication_mode</code> is a one-way decision. See announcement blog for further details:</p> <p>Switching authentication modes on an existing cluster is a one-way operation. You can switch from CONFIG_MAP to API_AND_CONFIG_MAP. You can then switch from API_AND_CONFIG_MAP to API. You cannot revert these operations in the opposite direction. Meaning you cannot switch back to CONFIG_MAP or API_AND_CONFIG_MAP from API.</p> <p>[!IMPORTANT] If migrating to cluster access entries and you will NOT have any entries that remain in the <code>aws-auth</code> ConfigMap, you do not need to remove the configmap from the statefile. You can simply follow the migration guide and once access entries have been created, you can let Terraform remove/delete the <code>aws-auth</code> ConfigMap.</p> <p>If you WILL have entries that remain in the <code>aws-auth</code> ConfigMap, then you will need to remove the ConfigMap resources from the statefile to avoid any disruptions. When you add the new <code>aws-auth</code> sub-module and apply the changes, the sub-module will upsert the ConfigMap on the cluster. Provided the necessary entries are defined in that sub-module's definition, it will \"re-adopt\" the ConfigMap under Terraform's control.</p>"},{"location":"UPGRADE-20.0/#authentication_mode-api_and_config_map","title":"authentication_mode = \"API_AND_CONFIG_MAP\"","text":"<p>When using <code>authentication_mode = \"API_AND_CONFIG_MAP\"</code> and there are entries that will remain in the configmap (entries that cannot be replaced by cluster access entry), you will first need to update the <code>authentication_mode</code> on the cluster to <code>\"API_AND_CONFIG_MAP\"</code>. To help make this upgrade process easier, a copy of the changes defined in the <code>v20.0.0</code> PR have been captured here but with the <code>aws-auth</code> components still provided in the module. This means you get the equivalent of the <code>v20.0.0</code> module, but it still includes support for the <code>aws-auth</code> configmap. You can follow the provided README on that interim migration module for the order of execution and return here once the <code>authentication_mode</code> has been updated to <code>\"API_AND_CONFIG_MAP\"</code>. Note - EKS automatically adds access entries for the roles used by EKS managed node groups and Fargate profiles; users do not need to do anything additional for these roles.</p> <p>Once the <code>authentication_mode</code> has been updated, next you will need to remove the configmap from the statefile to avoid any disruptions:</p> <p>[!NOTE] This is only required if there are entries that will remain in the <code>aws-auth</code> ConfigMap after migrating. Otherwise, you can skip this step and let Terraform destroy the ConfigMap.</p> <pre><code>terraform state rm 'module.eks.kubernetes_config_map_v1_data.aws_auth[0]'\nterraform state rm 'module.eks.kubernetes_config_map.aws_auth[0]' # include if Terraform created the original configmap\n</code></pre>"},{"location":"UPGRADE-20.0/#i-terraform-17-users","title":"\u2139\ufe0f Terraform 1.7+ users","text":"<p>If you are using Terraform <code>v1.7+</code>, you can utilize the <code>remove</code> to facilitate both the removal of the configmap through code. You can create a fork/clone of the provided migration module and add the <code>remove</code> blocks and apply those changes before proceeding. We do not want to force users onto the bleeding edge with this module, so we have not included <code>remove</code> support at this time.</p> <p>Once the configmap has been removed from the statefile, you can add the new <code>aws-auth</code> sub-module and copy the relevant definitions from the EKS module over to the new <code>aws-auth</code> sub-module definition (see before after diff above). When you apply the changes with the new sub-module, the configmap in the cluster will get updated with the contents provided in the sub-module definition, so please be sure all of the necessary entries are added before applying the changes. In the before/example above - the configmap would remove any entries for roles used by node groups and/or Fargate Profiles, but maintain the custom entries for users and roles passed into the module definition.</p>"},{"location":"UPGRADE-20.0/#authentication_mode-api","title":"authentication_mode = \"API\"","text":"<p>In order to switch to <code>API</code> only using cluster access entry, you first need to update the <code>authentication_mode</code> on the cluster to <code>API_AND_CONFIG_MAP</code> without modifying the <code>aws-auth</code> configmap. To help make this upgrade process easier, a copy of the changes defined in the <code>v20.0.0</code> PR have been captured here but with the <code>aws-auth</code> components still provided in the module. This means you get the equivalent of the <code>v20.0.0</code> module, but it still includes support for the <code>aws-auth</code> configmap. You can follow the provided README on that interim migration module for the order of execution and return here once the <code>authentication_mode</code> has been updated to <code>\"API_AND_CONFIG_MAP\"</code>. Note - EKS automatically adds access entries for the roles used by EKS managed node groups and Fargate profiles; users do not need to do anything additional for these roles.</p> <p>Once the <code>authentication_mode</code> has been updated, you can update the <code>authentication_mode</code> on the cluster to <code>API</code> and remove the <code>aws-auth</code> configmap components.</p>"},{"location":"UPGRADE-21.0/","title":"Upgrade from v20.x to v21.x","text":"<p>If you have any questions regarding this upgrade process, please consult the <code>examples</code> directory: If you find a bug, please open an issue with supporting configuration to reproduce.</p>"},{"location":"UPGRADE-21.0/#list-of-backwards-incompatible-changes","title":"List of backwards incompatible changes","text":"<ul> <li>Terraform <code>v1.5.7</code> is now minimum supported version</li> <li>AWS provider <code>v6.0.0</code> is now minimum supported version</li> <li>TLS provider <code>v4.0.0</code> is now minimum supported version</li> <li>The <code>aws-auth</code> sub-module has been removed. Users who wish to utilize its functionality can continue to do so by specifying a <code>v20.x</code> version, or <code>~&gt; v20.0</code> version constraint in their module source.</li> <li><code>bootstrap_self_managed_addons</code> is now hardcoded to <code>false</code>. This is a legacy setting and instead users should utilize the EKS addons API, which is what this module does by default. In conjunction with this change, the <code>bootstrap_self_managed_addons</code> is now ignored by the module to aid in upgrading without disruption (otherwise it would require cluster re-creation).</li> <li>When enabling <code>enable_efa_support</code> or creating placement groups within a node group, users must now specify the correct <code>subnet_ids</code>; the module no longer tries to automatically select a suitable subnet.</li> <li>EKS managed node group:<ul> <li>IMDS now default to a hop limit of 1 (previously was 2)</li> <li><code>ami_type</code> now defaults to <code>AL2023_x86_64_STANDARD</code></li> <li><code>enable_monitoring</code> is now set to <code>false</code> by default</li> <li><code>enable_efa_only</code> is now set to <code>true</code> by default</li> <li><code>use_latest_ami_release_version</code> is now set to <code>true</code> by default</li> <li>Support for autoscaling group schedules has been removed</li> </ul> </li> <li>Self-managed node group:<ul> <li>IMDS now default to a hop limit of 1 (previously was 2)</li> <li><code>ami_type</code> now defaults to <code>AL2023_x86_64_STANDARD</code></li> <li><code>enable_monitoring</code> is now set to <code>false</code> by default</li> <li><code>enable_efa_only</code> is now set to <code>true</code> by default</li> <li>Support for autoscaling group schedules has been removed</li> </ul> </li> <li>Karpenter:<ul> <li>Native support for IAM roles for service accounts (IRSA) has been removed; EKS Pod Identity is now enabled by default</li> <li>Karpenter controller policy for prior to Karpenter <code>v1</code> have been removed (i.e. <code>v0.33</code>); the <code>v1</code> policy is now used by default</li> <li><code>create_pod_identity_association</code> is now set to <code>true</code> by default</li> </ul> </li> <li><code>addons.resolve_conflicts_on_create</code> is now set to <code>\"NONE\"</code> by default (was <code>\"OVERWRITE\"</code>).</li> <li><code>addons.most_recent</code> is now set to <code>true</code> by default (was <code>false</code>).</li> <li><code>cluster_identity_providers.issuer_url</code> is now required to be set by users; the prior incorrect default has been removed. See https://github.com/terraform-aws-modules/terraform-aws-eks/pull/3055 and https://github.com/kubernetes/kubernetes/pull/123561 for more details.</li> <li>The OIDC issuer URL for IAM roles for service accounts (IRSA) has been changed to use the new dual stack<code>oidc-eks</code> endpoint instead of <code>oidc.eks</code>. This is to align with https://github.com/aws/containers-roadmap/issues/2038#issuecomment-2278450601</li> <li>With the changes to the variable type definition for <code>encryption_config</code> (formerly <code>cluster_encryption_config</code>), if you wish to disable secret encryption with a custom KMS key you should set <code>encryption_config = null</code> (In <code>v20.x</code>, you would previously have set <code>encryption_config = {}</code> to achieve the same outcome). Secret encryption can no longer be disabled - it is either enabled by default with the AWS managed key (<code>encryption_config = null</code>), or with a custom KMS key ( either leaving as is by not specifying or passing your own custom key ARN). EKS now encrypts secrets at rest by default  docs.aws.amazon.com/eks/latest/userguide/envelope-encryption.html and the default secret encryption w/ custom KMS key creation/usage by default was made years prior starting in version <code>v19.0</code> of this module. Removing this default behavior will be evaluated at the next breaking change given that secrets are now automatically encrypted at rest by AWS.</li> </ul>"},{"location":"UPGRADE-21.0/#additional-changes","title":"Additional changes","text":""},{"location":"UPGRADE-21.0/#added","title":"Added","text":"<ul> <li>Support for <code>region</code> parameter to specify the AWS region for the resources created if different from the provider region.</li> <li>Both the EKS managed and self-managed node groups now support creating their own security groups (again). This is primarily motivated by the changes for EFA support; previously users would need to specify <code>enable_efa_support</code> both at the cluster level (to add the appropriate security group rules to the shared node security group) as well as the node group level. However, its not always desirable to have these rules across ALL node groups when they are really only required on the node group where EFA is utilized. And similarly for other use cases, users can create custom rules for a specific node group instead of apply across ALL node groups.</li> </ul>"},{"location":"UPGRADE-21.0/#modified","title":"Modified","text":"<ul> <li>Variable definitions now contain detailed <code>object</code> types in place of the previously used any type.</li> <li>The embedded KMS key module definition has been updated to <code>v4.0</code> to support the same version requirements as well as the new <code>region</code> argument.</li> </ul>"},{"location":"UPGRADE-21.0/#variable-and-output-changes","title":"Variable and output changes","text":"<ol> <li> <p>Removed variables:</p> <ul> <li><code>enable_efa_support</code> - users only need to set this within the node group configuration, as the module no longer manages EFA support at the cluster level.</li> <li><code>enable_security_groups_for_pods</code> - users can instead attach the <code>arn:aws:iam::aws:policy/AmazonEKSVPCResourceController</code> policy via <code>iam_role_additional_policies</code> if using security groups for pods.</li> <li><code>eks-managed-node-group</code> sub-module<ul> <li><code>cluster_service_ipv4_cidr</code> - users should use <code>cluster_service_cidr</code> instead (for either IPv4 or IPv6).</li> <li><code>elastic_gpu_specifications</code></li> <li><code>elastic_inference_accelerator</code></li> <li><code>platform</code> - this is superseded by <code>ami_type</code></li> <li><code>placement_group_strategy</code> - set to <code>cluster</code> by the module</li> <li><code>placement_group_az</code> - users will need to specify the correct subnet in <code>subnet_ids</code></li> <li><code>create_schedule</code></li> <li><code>schedules</code></li> </ul> </li> <li><code>self-managed-node-group</code> sub-module<ul> <li><code>elastic_gpu_specifications</code></li> <li><code>elastic_inference_accelerator</code></li> <li><code>platform</code> - this is superseded by <code>ami_type</code></li> <li><code>create_schedule</code></li> <li><code>schedules</code></li> <li><code>placement_group_az</code> - users will need to specify the correct subnet in <code>subnet_ids</code></li> <li><code>hibernation_options</code> - not valid in EKS</li> <li><code>min_elb_capacity</code> - not valid in EKS</li> <li><code>wait_for_elb_capacity</code> - not valid in EKS</li> <li><code>wait_for_capacity_timeout</code> - not valid in EKS</li> <li><code>default_cooldown</code> - not valid in EKS</li> <li><code>target_group_arns</code> - not valid in EKS</li> <li><code>service_linked_role_arn</code> - not valid in EKS</li> <li><code>warm_pool</code> - not valid in EKS</li> </ul> </li> <li><code>fargate-profile</code> sub-module<ul> <li>None</li> </ul> </li> <li><code>karpenter</code> sub-module<ul> <li><code>enable_v1_permissions</code> - v1 permissions are now the default</li> <li><code>enable_irsa</code></li> <li><code>irsa_oidc_provider_arn</code></li> <li><code>irsa_namespace_service_accounts</code></li> <li><code>irsa_assume_role_condition_test</code></li> </ul> </li> <li><code>self-managed-node-group-defaults</code></li> </ul> </li> <li> <p>Renamed variables:</p> <ul> <li>Variables prefixed with <code>cluster_*</code> have been stripped of the prefix to better match the underlying API:<ul> <li><code>cluster_name</code> -&gt; <code>name</code></li> <li><code>cluster_version</code> -&gt; <code>kubernetes_version</code></li> <li><code>cluster_enabled_log_types</code> -&gt; <code>enabled_log_types</code></li> <li><code>cluster_force_update_version</code> -&gt; <code>force_update_version</code></li> <li><code>cluster_compute_config</code> -&gt; <code>compute_config</code></li> <li><code>cluster_upgrade_policy</code> -&gt; <code>upgrade_policy</code></li> <li><code>cluster_remote_network_config</code> -&gt; <code>remote_network_config</code></li> <li><code>cluster_zonal_shift_config</code> -&gt; <code>zonal_shift_config</code></li> <li><code>cluster_additional_security_group_ids</code> -&gt; <code>additional_security_group_ids</code></li> <li><code>cluster_endpoint_private_access</code> -&gt; <code>endpoint_private_access</code></li> <li><code>cluster_endpoint_public_access</code> -&gt; <code>endpoint_public_access</code></li> <li><code>cluster_endpoint_public_access_cidrs</code> -&gt; <code>endpoint_public_access_cidrs</code></li> <li><code>cluster_ip_family</code> -&gt; <code>ip_family</code></li> <li><code>cluster_service_ipv4_cidr</code> -&gt; <code>service_ipv4_cidr</code></li> <li><code>cluster_service_ipv6_cidr</code> -&gt; <code>service_ipv6_cidr</code></li> <li><code>cluster_encryption_config</code> -&gt; <code>encryption_config</code></li> <li><code>create_cluster_primary_security_group_tags</code> -&gt; <code>create_primary_security_group_tags</code></li> <li><code>cluster_timeouts</code> -&gt; <code>timeouts</code></li> <li><code>create_cluster_security_group</code> -&gt; <code>create_security_group</code></li> <li><code>cluster_security_group_id</code> -&gt; <code>security_group_id</code></li> <li><code>cluster_security_group_name</code> -&gt; <code>security_group_name</code></li> <li><code>cluster_security_group_use_name_prefix</code> -&gt; <code>security_group_use_name_prefix</code></li> <li><code>cluster_security_group_description</code> -&gt; <code>security_group_description</code></li> <li><code>cluster_security_group_additional_rules</code> -&gt; <code>security_group_additional_rules</code></li> <li><code>cluster_security_group_tags</code> -&gt; <code>security_group_tags</code></li> <li><code>cluster_encryption_policy_use_name_prefix</code> -&gt; <code>encryption_policy_use_name_prefix</code></li> <li><code>cluster_encryption_policy_name</code> -&gt; <code>encryption_policy_name</code></li> <li><code>cluster_encryption_policy_description</code> -&gt; <code>encryption_policy_description</code></li> <li><code>cluster_encryption_policy_path</code> -&gt; <code>encryption_policy_path</code></li> <li><code>cluster_encryption_policy_tags</code> -&gt; <code>encryption_policy_tags</code></li> <li><code>cluster_addons</code> -&gt; <code>addons</code></li> <li><code>cluster_addons_timeouts</code> -&gt; <code>addons_timeouts</code></li> <li><code>cluster_identity_providers</code> -&gt; <code>identity_providers</code></li> </ul> </li> <li><code>eks-managed-node-group</code> sub-module<ul> <li><code>cluster_version</code> -&gt; <code>kubernetes_version</code></li> </ul> </li> <li><code>self-managed-node-group</code> sub-module<ul> <li><code>cluster_version</code> -&gt; <code>kubernetes_version</code></li> <li><code>delete_timeout</code> -&gt; <code>timeouts</code></li> </ul> </li> <li><code>fargate-profile</code> sub-module<ul> <li>None</li> </ul> </li> <li><code>karpenter</code> sub-module<ul> <li>None</li> </ul> </li> </ul> </li> <li> <p>Added variables:</p> <ul> <li><code>region</code></li> <li><code>eks-managed-node-group</code> sub-module<ul> <li><code>region</code></li> <li><code>partition</code> - added to reduce number of <code>GET</code> requests from data sources when possible</li> <li><code>account_id</code> - added to reduce number of <code>GET</code> requests from data sources when possible</li> <li><code>create_security_group</code></li> <li><code>security_group_name</code></li> <li><code>security_group_use_name_prefix</code></li> <li><code>security_group_description</code></li> <li><code>security_group_ingress_rules</code></li> <li><code>security_group_egress_rules</code></li> <li><code>security_group_tags</code></li> </ul> </li> <li><code>self-managed-node-group</code> sub-module<ul> <li><code>region</code></li> <li><code>partition</code> - added to reduce number of <code>GET</code> requests from data sources when possible</li> <li><code>account_id</code> - added to reduce number of <code>GET</code> requests from data sources when possible</li> <li><code>create_security_group</code></li> <li><code>security_group_name</code></li> <li><code>security_group_use_name_prefix</code></li> <li><code>security_group_description</code></li> <li><code>security_group_ingress_rules</code></li> <li><code>security_group_egress_rules</code></li> <li><code>security_group_tags</code></li> </ul> </li> <li><code>fargate-profile</code> sub-module<ul> <li><code>region</code></li> <li><code>partition</code> - added to reduce number of <code>GET</code> requests from data sources when possible</li> <li><code>account_id</code> - added to reduce number of <code>GET</code> requests from data sources when possible</li> </ul> </li> <li><code>karpenter</code> sub-module<ul> <li><code>region</code></li> </ul> </li> </ul> </li> <li> <p>Removed outputs:</p> <ul> <li><code>eks-managed-node-group</code> sub-module<ul> <li><code>platform</code> - this is superseded by <code>ami_type</code></li> <li><code>autoscaling_group_schedule_arns</code></li> </ul> </li> <li><code>self-managed-node-group</code> sub-module<ul> <li><code>platform</code> - this is superseded by <code>ami_type</code></li> <li><code>autoscaling_group_schedule_arns</code></li> </ul> </li> <li><code>fargate-profile</code> sub-module<ul> <li>None</li> </ul> </li> <li><code>karpenter</code> sub-module<ul> <li>None</li> </ul> </li> </ul> </li> <li> <p>Renamed outputs:</p> <ul> <li><code>eks-managed-node-group</code> sub-module<ul> <li>None</li> </ul> </li> <li><code>self-managed-node-group</code> sub-module<ul> <li>None</li> </ul> </li> <li><code>fargate-profile</code> sub-module<ul> <li>None</li> </ul> </li> <li><code>karpenter</code> sub-module<ul> <li>None</li> </ul> </li> </ul> </li> <li> <p>Added outputs:</p> <ul> <li><code>eks-managed-node-group</code> sub-module<ul> <li><code>security_group_arn</code></li> <li><code>security_group_id</code></li> </ul> </li> <li><code>self-managed-node-group</code> sub-module<ul> <li><code>security_group_arn</code></li> <li><code>security_group_id</code></li> </ul> </li> <li><code>fargate-profile</code> sub-module<ul> <li>None</li> </ul> </li> <li><code>karpenter</code> sub-module<ul> <li>None</li> </ul> </li> </ul> </li> </ol>"},{"location":"UPGRADE-21.0/#upgrade-migrations","title":"Upgrade Migrations","text":""},{"location":"UPGRADE-21.0/#before-20x-example","title":"Before 20.x Example","text":"<pre><code>module \"eks\" {\n  source  = \"terraform-aws-modules/eks/aws\"\n  version = \"~&gt; 20.0\"\n\n  # Truncated for brevity ...\n  # Renamed variables are not shown here, please refer to the full list above.\n\n  enable_efa_support = true\n\n  eks_managed_node_group_defaults = {\n    iam_role_additional_policies = {\n      AmazonSSMManagedInstanceCore = \"arn:aws:iam::aws:policy/AmazonSSMManagedInstanceCore\"\n    }\n  }\n\n  eks_managed_node_groups = {\n    efa = {\n      ami_type       = \"AL2023_x86_64_NVIDIA\"\n      instance_types = [\"p5e.48xlarge\"]\n\n      enable_efa_support = true\n      enable_efa_only    = true\n    }\n  }\n\n  self_managed_node_groups = {\n    example = {\n      use_mixed_instances_policy = true\n      mixed_instances_policy = {\n        instances_distribution = {\n          on_demand_base_capacity                  = 0\n          on_demand_percentage_above_base_capacity = 0\n          on_demand_allocation_strategy            = \"lowest-price\"\n          spot_allocation_strategy                 = \"price-capacity-optimized\"\n        }\n\n        # ASG configuration\n        override = [\n          {\n            instance_requirements = {\n              cpu_manufacturers                           = [\"intel\"]\n              instance_generations                        = [\"current\", \"previous\"]\n              spot_max_price_percentage_over_lowest_price = 100\n\n              vcpu_count = {\n                min = 1\n              }\n\n              allowed_instance_types = [\"t*\", \"m*\"]\n            }\n          }\n        ]\n      }\n    }\n  }\n}\n</code></pre>"},{"location":"UPGRADE-21.0/#after-21x-example","title":"After 21.x Example","text":"<pre><code>module \"eks\" {\n  source  = \"terraform-aws-modules/eks/aws\"\n  version = \"~&gt; 21.0\"\n\n  # Truncated for brevity ...\n  # Renamed variables are not shown here, please refer to the full list above.\n\n  eks_managed_node_groups = {\n    efa = {\n      ami_type       = \"AL2023_x86_64_NVIDIA\"\n      instance_types = [\"p5e.48xlarge\"]\n\n      iam_role_additional_policies = {\n        AmazonSSMManagedInstanceCore = \"arn:aws:iam::aws:policy/AmazonSSMManagedInstanceCore\"\n      }\n\n      enable_efa_support = true\n\n      subnet_ids = element(module.vpc.private_subnets, 0)\n    }\n  }\n\n  self_managed_node_groups = {\n    example = {\n      use_mixed_instances_policy = true\n      mixed_instances_policy = {\n        instances_distribution = {\n          on_demand_base_capacity                  = 0\n          on_demand_percentage_above_base_capacity = 0\n          on_demand_allocation_strategy            = \"lowest-price\"\n          spot_allocation_strategy                 = \"price-capacity-optimized\"\n        }\n\n        # ASG configuration\n        # Need to wrap in `launch_template` now\n        launch_template = {\n          override = [\n            {\n              instance_requirements = {\n                cpu_manufacturers                           = [\"intel\"]\n                instance_generations                        = [\"current\", \"previous\"]\n                spot_max_price_percentage_over_lowest_price = 100\n\n                vcpu_count = {\n                  min = 1\n                }\n\n                allowed_instance_types = [\"t*\", \"m*\"]\n              }\n            }\n          ]\n        }\n      }\n    }\n  }\n}\n</code></pre>"},{"location":"UPGRADE-21.0/#state-changes","title":"State Changes","text":"<p>No state changes required.</p>"},{"location":"compute_resources/","title":"Compute Resources","text":""},{"location":"compute_resources/#table-of-contents","title":"Table of Contents","text":"<ul> <li>EKS Managed Node Groups</li> <li>Self Managed Node Groups</li> <li>Fargate Profiles</li> <li>Default Configurations</li> </ul> <p>\u2139\ufe0f Only the pertinent attributes are shown below for brevity</p>"},{"location":"compute_resources/#eks-managed-node-groups","title":"EKS Managed Node Groups","text":"<p>Refer to the EKS Managed Node Group documentation documentation for service related details.</p> <ol> <li>The module creates a custom launch template by default to ensure settings such as tags are propagated to instances. Please note that many of the customization options listed here are only available when a custom launch template is created. To use the default template provided by the AWS EKS managed node group service, disable the launch template creation by setting <code>use_custom_launch_template</code> to <code>false</code>:</li> </ol> <pre><code>  eks_managed_node_groups = {\n    default = {\n      use_custom_launch_template = false\n    }\n  }\n</code></pre> <ol> <li>Native support for Bottlerocket OS is provided by providing the respective AMI type:</li> </ol> <pre><code>  eks_managed_node_groups = {\n    bottlerocket_default = {\n      use_custom_launch_template = false\n\n      ami_type = \"BOTTLEROCKET_x86_64\"\n    }\n  }\n</code></pre> <ol> <li>Bottlerocket OS is supported in a similar manner. However, note that the user data for Bottlerocket OS uses the TOML format:</li> </ol> <pre><code>  eks_managed_node_groups = {\n    bottlerocket_prepend_userdata = {\n      ami_type = \"BOTTLEROCKET_x86_64\"\n\n      bootstrap_extra_args = &lt;&lt;-EOT\n        # extra args added\n        [settings.kernel]\n        lockdown = \"integrity\"\n      EOT\n    }\n  }\n</code></pre> <ol> <li>When using a custom AMI, the AWS EKS Managed Node Group service will NOT inject the necessary bootstrap script into the supplied user data. Users can elect to provide their own user data to bootstrap and connect or opt in to use the module provided user data:</li> </ol> <pre><code>  eks_managed_node_groups = {\n    custom_ami = {\n      ami_id   = \"ami-0caf35bc73450c396\"\n      ami_type = \"AL2023_x86_64_STANDARD\"\n\n      # By default, EKS managed node groups will not append bootstrap script;\n      # this adds it back in using the default template provided by the module\n      # Note: this assumes the AMI provided is an EKS optimized AMI derivative\n      enable_bootstrap_user_data = true\n\n      cloudinit_pre_nodeadm = [{\n        content      = &lt;&lt;-EOT\n          ---\n          apiVersion: node.eks.aws/v1alpha1\n          kind: NodeConfig\n          spec:\n            kubelet:\n              config:\n                shutdownGracePeriod: 30s\n        EOT\n        content_type = \"application/node.eks.aws\"\n      }]\n\n      # This is only possible when `ami_id` is specified, indicating a custom AMI\n      cloudinit_post_nodeadm = [{\n        content      = &lt;&lt;-EOT\n          echo \"All done\"\n        EOT\n        content_type = \"text/x-shellscript; charset=\\\"us-ascii\\\"\"\n      }]\n    }\n  }\n</code></pre> <ol> <li>There is similar support for Bottlerocket OS:</li> </ol> <pre><code>  eks_managed_node_groups = {\n    bottlerocket_custom_ami = {\n      ami_id   = \"ami-0ff61e0bcfc81dc94\"\n      ami_type = \"BOTTLEROCKET_x86_64\"\n\n      # use module user data template to bootstrap\n      enable_bootstrap_user_data = true\n      # this will get added to the template\n      bootstrap_extra_args = &lt;&lt;-EOT\n        # extra args added\n        [settings.kernel]\n        lockdown = \"integrity\"\n\n        [settings.kubernetes.node-labels]\n        \"label1\" = \"foo\"\n        \"label2\" = \"bar\"\n\n        [settings.kubernetes.node-taints]\n        \"dedicated\" = \"experimental:PreferNoSchedule\"\n        \"special\" = \"true:NoSchedule\"\n      EOT\n    }\n  }\n</code></pre> <p>See the <code>examples/eks-managed-node-group/</code> example for a working example of various configurations.</p>"},{"location":"compute_resources/#self-managed-node-groups","title":"Self Managed Node Groups","text":"<p>Refer to the Self Managed Node Group documentation documentation for service related details.</p> <ol> <li>The <code>self-managed-node-group</code> uses the latest AWS EKS Optimized AMI (Linux) for the given Kubernetes version by default:</li> </ol> <pre><code>  kubernetes_version = \"1.33\"\n\n  # This self managed node group will use the latest AWS EKS Optimized AMI for Kubernetes 1.33\n  self_managed_node_groups = {\n    default = {}\n  }\n</code></pre> <ol> <li>To use Bottlerocket, specify the <code>ami_type</code> as one of the respective <code>\"BOTTLEROCKET_*\" types</code> and supply a Bottlerocket OS AMI:</li> </ol> <pre><code>  kubernetes_version = \"1.33\"\n\n  self_managed_node_groups = {\n    bottlerocket = {\n      ami_id   = data.aws_ami.bottlerocket_ami.id\n      ami_type = \"BOTTLEROCKET_x86_64\"\n    }\n  }\n</code></pre> <p>See the <code>examples/self-managed-node-group/</code> example for a working example of various configurations.</p>"},{"location":"compute_resources/#fargate-profiles","title":"Fargate Profiles","text":"<p>Fargate profiles are straightforward to use and therefore no further details are provided here. See the <code>tests/eks-fargate-profile/</code> tests for a working example of various configurations.</p>"},{"location":"faq/","title":"Frequently Asked Questions","text":"<ul> <li>Setting <code>disk_size</code> or <code>remote_access</code> does not make any changes</li> <li>I received an error: <code>expect exactly one securityGroup tagged with kubernetes.io/cluster/&lt;NAME&gt; ...</code></li> <li>Why are nodes not being registered?</li> <li>Why are there no changes when a node group's <code>desired_size</code> is modified?</li> <li>How do I access compute resource attributes?</li> <li>What add-ons are available?</li> <li>What configuration values are available for an add-on?</li> </ul>"},{"location":"faq/#setting-disk_size-or-remote_access-does-not-make-any-changes","title":"Setting <code>disk_size</code> or <code>remote_access</code> does not make any changes","text":"<p><code>disk_size</code>, and <code>remote_access</code> can only be set when using the EKS managed node group default launch template. This module defaults to providing a custom launch template to allow for custom security groups, tag propagation, etc. If you wish to forgo the custom launch template route, you can set <code>use_custom_launch_template = false</code> and then you can set <code>disk_size</code> and <code>remote_access</code>.</p>"},{"location":"faq/#i-received-an-error-expect-exactly-one-securitygroup-tagged-with-kubernetesioclustercluster_name","title":"I received an error: <code>expect exactly one securityGroup tagged with kubernetes.io/cluster/&lt;CLUSTER_NAME&gt; ...</code>","text":"<p>\u26a0\ufe0f <code>&lt;CLUSTER_NAME&gt;</code> would be the name of your cluster</p> <p>By default, EKS creates a cluster primary security group that is created outside of the module and the EKS service adds the tag <code>{ \"kubernetes.io/cluster/&lt;CLUSTER_NAME&gt;\" = \"owned\" }</code>. This on its own does not cause any conflicts for addons such as the AWS Load Balancer Controller until users decide to attach both the cluster primary security group and the shared node security group created by the module (by setting <code>attach_cluster_primary_security_group = true</code>). The issue is not with having multiple security groups in your account with this tag key:value combination, but having multiple security groups with this tag key:value combination attached to nodes in the same cluster. There are a few ways to resolve this depending on your use case/intentions:</p> <ol> <li>If you want to use the cluster primary security group, you can disable the creation of the shared node security group with:</li> </ol> <pre><code>  create_node_security_group = false # default is true\n\n  eks_managed_node_group = {\n    example = {\n      attach_cluster_primary_security_group = true # default is false\n    }\n  }\n  # Or for self-managed\n  self_managed_node_group = {\n    example = {\n      attach_cluster_primary_security_group = true # default is false\n    }\n  }\n</code></pre> <ol> <li>By not attaching the cluster primary security group. The cluster primary security group has quite broad access and the module has instead provided a security group with the minimum amount of access to launch an empty EKS cluster successfully and users are encouraged to open up access when necessary to support their workload.</li> </ol> <pre><code>  eks_managed_node_group = {\n    example = {\n      attach_cluster_primary_security_group = true # default is false\n    }\n  }\n  # Or for self-managed\n  self_managed_node_group = {\n    example = {\n      attach_cluster_primary_security_group = true # default is false\n    }\n  }\n</code></pre> <p>In theory, if you are attaching the cluster primary security group, you shouldn't need to use the shared node security group created by the module. However, this is left up to users to decide for their requirements and use case.</p> <p>If you choose to use Custom Networking, make sure to only attach the security groups matching your choice above in your ENIConfig resources. This will ensure you avoid redundant tags.</p>"},{"location":"faq/#why-are-nodes-not-being-registered","title":"Why are nodes not being registered?","text":"<p>Nodes not being able to register with the EKS control plane is generally due to networking mis-configurations.</p> <ol> <li>At least one of the cluster endpoints (public or private) must be enabled.</li> </ol> <p>If you require a public endpoint, setting up both (public and private) and restricting the public endpoint via setting <code>cluster_endpoint_public_access_cidrs</code> is recommended. More info regarding communication with an endpoint is available here.</p> <ol> <li> <p>Nodes need to be able to contact the EKS cluster endpoint. By default, the module only creates a public endpoint. To access the endpoint, the nodes need outgoing internet access:</p> </li> <li> <p>Nodes in private subnets: via a NAT gateway or instance along with the appropriate routing rules</p> </li> <li>Nodes in public subnets: ensure that nodes are launched with public IPs (enable through either the module here or your subnet setting defaults)</li> </ol> <p>Important: If you apply only the public endpoint and configure the <code>cluster_endpoint_public_access_cidrs</code> to restrict access, know that EKS nodes will also use the public endpoint and you must allow access to the endpoint. If not, then your nodes will fail to work correctly.</p> <ol> <li> <p>The private endpoint can also be enabled by setting <code>cluster_endpoint_private_access = true</code>. Ensure that VPC DNS resolution and hostnames are also enabled for your VPC when the private endpoint is enabled.</p> </li> <li> <p>Nodes need to be able to connect to other AWS services to function (download container images, make API calls to assume roles, etc.). If for some reason you cannot enable public internet access for nodes you can add VPC endpoints to the relevant services: EC2 API, ECR API, ECR DKR and S3.</p> </li> </ol>"},{"location":"faq/#why-are-there-no-changes-when-a-node-groups-desired_size-is-modified","title":"Why are there no changes when a node group's <code>desired_size</code> is modified?","text":"<p>The module is configured to ignore this value. Unfortunately, Terraform does not support variables within the <code>lifecycle</code> block. The setting is ignored to allow autoscaling via controllers such as cluster autoscaler or Karpenter to work properly and without interference by Terraform. Changing the desired count must be handled outside of Terraform once the node group is created.</p> <p>:info: See this for a workaround to this limitation.</p>"},{"location":"faq/#how-do-i-access-compute-resource-attributes","title":"How do I access compute resource attributes?","text":"<p>Examples of accessing the attributes of the compute resource(s) created by the root module are shown below. Note - the assumption is that your cluster module definition is named <code>eks</code> as in <code>module \"eks\" { ... }</code>:</p> <ul> <li>EKS Managed Node Group attributes</li> </ul> <pre><code>eks_managed_role_arns = [for group in module.eks_managed_node_group : group.iam_role_arn]\n</code></pre> <ul> <li>Self Managed Node Group attributes</li> </ul> <pre><code>self_managed_role_arns = [for group in module.self_managed_node_group : group.iam_role_arn]\n</code></pre> <ul> <li>Fargate Profile attributes</li> </ul> <pre><code>fargate_profile_pod_execution_role_arns = [for group in module.fargate_profile : group.fargate_profile_pod_execution_role_arn]\n</code></pre>"},{"location":"faq/#what-add-ons-are-available","title":"What add-ons are available?","text":"<p>The available EKS add-ons can be found here. You can also retrieve the available addons from the API using:</p> <pre><code>aws eks describe-addon-versions --query 'addons[*].addonName'\n</code></pre>"},{"location":"faq/#what-configuration-values-are-available-for-an-add-on","title":"What configuration values are available for an add-on?","text":"<p>[!NOTE] The available configuration values will vary between add-on versions, typically more configuration values will be added in later versions as functionality is enabled by EKS.</p> <p>You can retrieve the configuration value schema for a given addon using the following command:</p> <pre><code>aws eks describe-addon-configuration --addon-name &lt;value&gt; --addon-version &lt;value&gt; --query 'configurationSchema' --output text | jq\n</code></pre> <p>For example:</p> <pre><code>aws eks describe-addon-configuration --addon-name coredns --addon-version v1.11.1-eksbuild.8 --query 'configurationSchema' --output text | jq\n</code></pre> <p>Returns (at the time of writing):</p> <pre><code>{\n  \"$ref\": \"#/definitions/Coredns\",\n  \"$schema\": \"http://json-schema.org/draft-06/schema#\",\n  \"definitions\": {\n    \"Coredns\": {\n      \"additionalProperties\": false,\n      \"properties\": {\n        \"affinity\": {\n          \"default\": {\n            \"affinity\": {\n              \"nodeAffinity\": {\n                \"requiredDuringSchedulingIgnoredDuringExecution\": {\n                  \"nodeSelectorTerms\": [\n                    {\n                      \"matchExpressions\": [\n                        {\n                          \"key\": \"kubernetes.io/os\",\n                          \"operator\": \"In\",\n                          \"values\": [\n                            \"linux\"\n                          ]\n                        },\n                        {\n                          \"key\": \"kubernetes.io/arch\",\n                          \"operator\": \"In\",\n                          \"values\": [\n                            \"amd64\",\n                            \"arm64\"\n                          ]\n                        }\n                      ]\n                    }\n                  ]\n                }\n              },\n              \"podAntiAffinity\": {\n                \"preferredDuringSchedulingIgnoredDuringExecution\": [\n                  {\n                    \"podAffinityTerm\": {\n                      \"labelSelector\": {\n                        \"matchExpressions\": [\n                          {\n                            \"key\": \"k8s-app\",\n                            \"operator\": \"In\",\n                            \"values\": [\n                              \"kube-dns\"\n                            ]\n                          }\n                        ]\n                      },\n                      \"topologyKey\": \"kubernetes.io/hostname\"\n                    },\n                    \"weight\": 100\n                  }\n                ]\n              }\n            }\n          },\n          \"description\": \"Affinity of the coredns pods\",\n          \"type\": [\n            \"object\",\n            \"null\"\n          ]\n        },\n        \"computeType\": {\n          \"type\": \"string\"\n        },\n        \"corefile\": {\n          \"description\": \"Entire corefile contents to use with installation\",\n          \"type\": \"string\"\n        },\n        \"nodeSelector\": {\n          \"additionalProperties\": {\n            \"type\": \"string\"\n          },\n          \"type\": \"object\"\n        },\n        \"podAnnotations\": {\n          \"properties\": {},\n          \"title\": \"The podAnnotations Schema\",\n          \"type\": \"object\"\n        },\n        \"podDisruptionBudget\": {\n          \"description\": \"podDisruptionBudget configurations\",\n          \"enabled\": {\n            \"default\": true,\n            \"description\": \"the option to enable managed PDB\",\n            \"type\": \"boolean\"\n          },\n          \"maxUnavailable\": {\n            \"anyOf\": [\n              {\n                \"pattern\": \".*%$\",\n                \"type\": \"string\"\n              },\n              {\n                \"type\": \"integer\"\n              }\n            ],\n            \"default\": 1,\n            \"description\": \"minAvailable value for managed PDB, can be either string or integer; if it's string, should end with %\"\n          },\n          \"minAvailable\": {\n            \"anyOf\": [\n              {\n                \"pattern\": \".*%$\",\n                \"type\": \"string\"\n              },\n              {\n                \"type\": \"integer\"\n              }\n            ],\n            \"description\": \"maxUnavailable value for managed PDB, can be either string or integer; if it's string, should end with %\"\n          },\n          \"type\": \"object\"\n        },\n        \"podLabels\": {\n          \"properties\": {},\n          \"title\": \"The podLabels Schema\",\n          \"type\": \"object\"\n        },\n        \"replicaCount\": {\n          \"type\": \"integer\"\n        },\n        \"resources\": {\n          \"$ref\": \"#/definitions/Resources\"\n        },\n        \"tolerations\": {\n          \"default\": [\n            {\n              \"key\": \"CriticalAddonsOnly\",\n              \"operator\": \"Exists\"\n            },\n            {\n              \"effect\": \"NoSchedule\",\n              \"key\": \"node-role.kubernetes.io/control-plane\"\n            }\n          ],\n          \"description\": \"Tolerations of the coredns pod\",\n          \"items\": {\n            \"type\": \"object\"\n          },\n          \"type\": \"array\"\n        },\n        \"topologySpreadConstraints\": {\n          \"description\": \"The coredns pod topology spread constraints\",\n          \"type\": \"array\"\n        }\n      },\n      \"title\": \"Coredns\",\n      \"type\": \"object\"\n    },\n    \"Limits\": {\n      \"additionalProperties\": false,\n      \"properties\": {\n        \"cpu\": {\n          \"type\": \"string\"\n        },\n        \"memory\": {\n          \"type\": \"string\"\n        }\n      },\n      \"title\": \"Limits\",\n      \"type\": \"object\"\n    },\n    \"Resources\": {\n      \"additionalProperties\": false,\n      \"properties\": {\n        \"limits\": {\n          \"$ref\": \"#/definitions/Limits\"\n        },\n        \"requests\": {\n          \"$ref\": \"#/definitions/Limits\"\n        }\n      },\n      \"title\": \"Resources\",\n      \"type\": \"object\"\n    }\n  }\n}\n</code></pre>"},{"location":"local/","title":"Local Development","text":""},{"location":"local/#documentation-site","title":"Documentation Site","text":"<p>In order to run the documentation site locally, you will need to have the following installed locally:</p> <ul> <li>Python 3.x</li> <li>mkdocs</li> <li>The following pip packages for mkdocs (i.e. - <code>pip install ...</code>)<ul> <li><code>mkdocs-material</code></li> <li><code>mkdocs-include-markdown-plugin</code></li> <li><code>mkdocs-awesome-pages-plugin</code></li> </ul> </li> </ul> <p>To run the documentation site locally, run the following command from the root of the repository:</p> <pre><code>mkdocs serve\n</code></pre> <p>Opening the documentation at the link posted in the terminal output (i.e. - http://127.0.0.1:8000/terraform-aws-eks/)</p>"},{"location":"network_connectivity/","title":"Network Connectivity","text":""},{"location":"network_connectivity/#cluster-endpoint","title":"Cluster Endpoint","text":""},{"location":"network_connectivity/#public-endpoint-w-restricted-cidrs","title":"Public Endpoint w/ Restricted CIDRs","text":"<p>When restricting the clusters public endpoint to only the CIDRs specified by users, it is recommended that you also enable the private endpoint, or ensure that the CIDR blocks that you specify include the addresses that nodes and Fargate pods (if you use them) access the public endpoint from.</p> <p>Please refer to the AWS documentation for further information</p>"},{"location":"network_connectivity/#security-groups","title":"Security Groups","text":"<ul> <li>Cluster Security Group</li> <li>This module by default creates a cluster security group (\"additional\" security group when viewed from the console) in addition to the default security group created by the AWS EKS service. This \"additional\" security group allows users to customize inbound and outbound rules via the module as they see fit<ul> <li>The default inbound/outbound rules provided by the module are derived from the AWS minimum recommendations in addition to NTP and HTTPS public internet egress rules (without, these show up in VPC flow logs as rejects - they are used for clock sync and downloading necessary packages/updates)</li> <li>The minimum inbound/outbound rules are provided for cluster and node creation to succeed without errors, but users will most likely need to add the necessary port and protocol for node-to-node communication (this is user specific based on how nodes are configured to communicate across the cluster)</li> <li>Users have the ability to opt out of the security group creation and instead provide their own externally created security group if so desired</li> <li>The security group that is created is designed to handle the bare minimum communication necessary between the control plane and the nodes, as well as any external egress to allow the cluster to successfully launch without error</li> </ul> </li> <li>Users also have the option to supply additional, externally created security groups to the cluster as well via the <code>cluster_additional_security_group_ids</code> variable</li> <li> <p>Lastly, users are able to opt in to attaching the primary security group automatically created by the EKS service by setting <code>attach_cluster_primary_security_group</code> = <code>true</code> from the root module for the respective node group (or set it within the node group defaults). This security group is not managed by the module; it is created by the EKS service. It permits all traffic within the domain of the security group as well as all egress traffic to the internet.</p> </li> <li> <p>Node Group Security Group(s)</p> </li> <li>Users have the option to assign their own externally created security group(s) to the node group via the <code>vpc_security_group_ids</code> variable</li> </ul> <p>See the example snippet below which adds additional security group rules to the cluster security group as well as the shared node security group (for node-to-node access). Users can use this extensibility to open up network access as they see fit using the security groups provided by the module:</p> <p><pre><code>  ...\n  # Extend cluster security group rules\n  security_group_additional_rules = {\n    egress_nodes_ephemeral_ports_tcp = {\n      description                = \"To node 1025-65535\"\n      protocol                   = \"tcp\"\n      from_port                  = 1025\n      to_port                    = 65535\n      type                       = \"egress\"\n      source_node_security_group = true\n    }\n  }\n\n  # Extend node-to-node security group rules\n  node_security_group_additional_rules = {\n    ingress_self_all = {\n      description = \"Node to node all ports/protocols\"\n      protocol    = \"-1\"\n      from_port   = 0\n      to_port     = 0\n      type        = \"ingress\"\n      self        = true\n    }\n    egress_all = {\n      description      = \"Node all egress\"\n      protocol         = \"-1\"\n      from_port        = 0\n      to_port          = 0\n      type             = \"egress\"\n      cidr_blocks      = [\"0.0.0.0/0\"]\n      ipv6_cidr_blocks = [\"::/0\"]\n    }\n  }\n  ...\n</code></pre> The security groups created by this module are depicted in the image shown below along with their default inbound/outbound rules:</p> <p> </p>"},{"location":"user_data/","title":"User Data &amp; Bootstrapping","text":"<p>Users can see the various methods of using and providing user data through the user data tests as well more detailed information on the design and possible configurations via the user data module itself</p>"},{"location":"user_data/#summary","title":"Summary","text":"<ul> <li>AWS EKS Managed Node Groups</li> <li>By default, any supplied user data is pre-pended to the user data supplied by the EKS Managed Node Group service</li> <li>If users supply an <code>ami_id</code>, the service no longers supplies user data to bootstrap nodes; users can enable <code>enable_bootstrap_user_data</code> and use the module provided user data template, or provide their own user data template</li> <li>AMI types of <code>BOTTLEROCKET_*</code>, user data must be in TOML format</li> <li>AMI types of <code>WINDOWS_*</code>, user data must be in powershell/PS1 script format</li> <li>Self Managed Node Groups</li> <li><code>AL2_*</code> AMI types -&gt; the user data template (bash/shell script) provided by the module is used as the default; users are able to provide their own user data template</li> <li><code>AL2023_*</code> AMI types -&gt; the user data template (MIME multipart format) provided by the module is used as the default; users are able to provide their own user data template</li> <li><code>BOTTLEROCKET_*</code> AMI types -&gt; the user data template (TOML file) provided by the module is used as the default; users are able to provide their own user data template</li> <li><code>WINDOWS_*</code> AMI types -&gt; the user data template (powershell/PS1 script) provided by the module is used as the default; users are able to provide their own user data template</li> </ul> <p>The templates provided by the module can be found under the templates directory</p>"},{"location":"user_data/#eks-managed-node-group","title":"EKS Managed Node Group","text":"<p>When using an EKS managed node group, users have 2 primary routes for interacting with the bootstrap user data:</p> <ol> <li> <p>If a value for <code>ami_id</code> is not provided, users can supply additional user data that is pre-pended before the EKS Managed Node Group bootstrap user data. You can read more about this process from the AWS supplied documentation</p> </li> <li> <p>Users can use the following variables to facilitate this process:</p> <p>For <code>AL2_*</code>, <code>BOTTLEROCKET_*</code>, and <code>WINDOWS_*</code>: <pre><code>pre_bootstrap_user_data = \"...\"\n</code></pre></p> <p>For <code>AL2023_*</code> <pre><code>cloudinit_pre_nodeadm = [{\n  content      = &lt;&lt;-EOT\n    ---\n    apiVersion: node.eks.aws/v1alpha1\n    kind: NodeConfig\n    spec:\n      ...\n  EOT\n  content_type = \"application/node.eks.aws\"\n}]\n</code></pre></p> </li> <li> <p>If a custom AMI is used, then per the AWS documentation, users will need to supply the necessary user data to bootstrap and register nodes with the cluster when launched. There are two routes to facilitate this bootstrapping process:</p> </li> <li>If the AMI used is a derivative of the AWS EKS Optimized AMI , users can opt in to using a template provided by the module that provides the minimum necessary configuration to bootstrap the node when launched:<ul> <li>Users can use the following variables to facilitate this process:    <pre><code>enable_bootstrap_user_data = true # to opt in to using the module supplied bootstrap user data template\npre_bootstrap_user_data    = \"...\"\nbootstrap_extra_args       = \"...\"\npost_bootstrap_user_data   = \"...\"\n</code></pre></li> </ul> </li> <li>If the AMI is NOT an AWS EKS Optimized AMI derivative, or if users wish to have more control over the user data that is supplied to the node when launched, users have the ability to supply their own user data template that will be rendered instead of the module supplied template. Note - only the variables that are supplied to the <code>templatefile()</code> for the respective AMI type are available for use in the supplied template, otherwise users will need to pre-render/pre-populate the template before supplying the final template to the module for rendering as user data.<ul> <li>Users can use the following variables to facilitate this process:    <pre><code>user_data_template_path  = \"./your/user_data.sh\" # user supplied bootstrap user data template\npre_bootstrap_user_data  = \"...\"\nbootstrap_extra_args     = \"...\"\npost_bootstrap_user_data = \"...\"\n</code></pre></li> </ul> </li> </ol> \u2139\ufe0f When using bottlerocket, the supplied user data (TOML format) is merged in with the values supplied by EKS. Therefore, <code>pre_bootstrap_user_data</code> and <code>post_bootstrap_user_data</code> are not valid since the bottlerocket OS handles when various settings are applied. If you wish to supply additional configuration settings when using bottlerocket, supply them via the <code>bootstrap_extra_args</code> variable. For the <code>AL2_*</code> AMI types, <code>bootstrap_extra_args</code> are settings that will be supplied to the AWS EKS Optimized AMI bootstrap script such as kubelet extra args, etc. See the bottlerocket GitHub repository documentation for more details on what settings can be supplied via the <code>bootstrap_extra_args</code> variable."},{"location":"user_data/#self-managed-node-group","title":"Self Managed Node Group","text":"<p>Self managed node groups require users to provide the necessary bootstrap user data. Users can elect to use the user data template provided by the module for their respective AMI type or provide their own user data template for rendering by the module.</p> <ul> <li>If the AMI used is a derivative of the AWS EKS Optimized AMI , users can opt in to using a template provided by the module that provides the minimum necessary configuration to bootstrap the node when launched:</li> <li>Users can use the following variables to facilitate this process:     <pre><code>enable_bootstrap_user_data = true # to opt in to using the module supplied bootstrap user data template\npre_bootstrap_user_data    = \"...\"\nbootstrap_extra_args       = \"...\"\npost_bootstrap_user_data   = \"...\"\n</code></pre></li> <li>If the AMI is NOT an AWS EKS Optimized AMI derivative, or if users wish to have more control over the user data that is supplied to the node when launched, users have the ability to supply their own user data template that will be rendered instead of the module supplied template. Note - only the variables that are supplied to the <code>templatefile()</code> for the respective AMI type are available for use in the supplied template, otherwise users will need to pre-render/pre-populate the template before supplying the final template to the module for rendering as user data.<ul> <li>Users can use the following variables to facilitate this process:   <pre><code>user_data_template_path  = \"./your/user_data.sh\" # user supplied bootstrap user data template\npre_bootstrap_user_data  = \"...\"\nbootstrap_extra_args     = \"...\"\npost_bootstrap_user_data = \"...\"\n</code></pre></li> </ul> </li> </ul>"},{"location":"user_data/#logic-diagram","title":"Logic Diagram","text":"<p>The rough flow of logic that is encapsulated within the <code>_user_data</code> module can be represented by the following diagram to better highlight the various manners in which user data can be populated.</p> <p> </p>"}]}